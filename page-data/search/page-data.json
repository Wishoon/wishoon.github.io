{"componentChunkName":"component---src-pages-search-jsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"nodes":[{"excerpt":"재고를 기반으로 운영되는 쿠폰 시스템은 개발자가 고려해야 할 점이 많습니다. 현재 개발되고 있는 프로젝트는 자바와 스프링을 기반으로 한 웹 어플리케이션으로 운용되고 있습니다. 이러한 어플리케이션은 기본적으로 멀티 스레드 환경에서 구동이 되기 때문에 공유 자원에 대한 경쟁 상태가 발생하지 않도록 주의를 기울여 개발을 해야 합니다. 이러한 문제를 해결하기 위…","fields":{"slug":"/동시성 문제 해결을 통한 제고 시스템 구현하기 - 1/"},"frontmatter":{"date":"November 16, 2022","title":"동시성 문제 해결을 통한 제고 시스템 구현하기 - 1","tags":["Transactional"]},"rawMarkdownBody":"\n재고를 기반으로 운영되는 쿠폰 시스템은 개발자가 고려해야 할 점이 많습니다. 현재 개발되고 있는 프로젝트는 자바와 스프링을 기반으로 한 웹 어플리케이션으로 운용되고 있습니다. 이러한 어플리케이션은 기본적으로 멀티 스레드 환경에서 구동이 되기 때문에 공유 자원에 대한 경쟁 상태가 발생하지 않도록 주의를 기울여 개발을 해야 합니다.\n\n이러한 문제를 해결하기 위해 우리는 다양한 부분에 대해서 고민을 해볼 필요성이 있습니다. 그 중의 하나가 데이터베이스의 상태를 변화시키는 작업의 단위라고 불리는 `트랜잭션` 이라고 생각합니다. 이번 글에서는 트랜잭션의 관점에서 어떻게 동시성 문제를 해결하는지 살펴보고, Spring에서는 어떻게 트랜잭션을 지원하는지 그 개념과 동작원리를 살펴보는 시간을 가져보려고 합니다.\n\n## Transactional의 개념과 격리레벨\n\n### 트랜잭션이란?\n\n**트랜잭션은 데이터베이스 상태를 변화시키는 하나의 논리적인 작업기능을 구성하는 단위**를 말합니다. 어떤 작업들이 성공적으로 완료되어야 구성된 작업의 결과를 반영하고, 오류가 발생했을 때는 이전에 있었던 모든 작업들이 성공적이라고 해도 없었던일 처럼 완전히 되돌리는 것이 트랜잭션의 개념이라고 할 수 있습니다.\n\n이러한 트랜잭션을 수행함에 있어서 동시에 여러 트랜잭션이 변경을 수행할 때 일관성을 유지하는 기능을 제공하고 안정성과 성능을 조절하기 위해서 필요한 설정이 바로 트랜잭션의 격리 레벨이라고 할 수 있습니다.\n\n### 트랜잭션의 격리 레벨이란 무엇인가?\n\n트랜잭션 격리 레벨이란 앞서서 말했듯이 **동시에 여러 트랜잭션이 처리될 때, 트랜잭션끼리 얼마나 서로 고립되어 있는지를 나타내는 것**입니다. 즉, 특정 트랜잭션이 다른 트랜잭션에 변경할 데이터를 볼 수 있도록 허용할지 말지를 결정합니다.\n\n이를 수행하기 위해서, 트랜잭션이 독립적인 수행을 하도록 `Locking`을 통해, 하나의 트랜잭션이 데이터베이스를 다루는 동안 다른 트랜잭션이 해당 트랜잭션에 관여하지 못하도록 막는 작업이 필요합니다.\n\n하지만 무조건적인 `Locking`으로 동시에 수행되는 수많은 트랜잭션을 순서대로 처리하도록 설계하게 되면 데이터베이스의 동시성 성능이 떨어지게 됩니다. 이러한 성능저하를 줄이기 위해서 `Locking`의 범위를 줄인다면, 데이터 일관성이 깨지는 문제가 발생할 수도 있습니다. 따라서 최대한 효율적으로 `Locking` 설정하는 것이 좋습니다.\n\n### Locking Reads\n\n기본적으로 `SELECT`문을 통해 데이터를 조회하게 되면 `Non-Locking` 상태라고 할 수 있습니다. 때문에 읽기 작업과 쓰기 작업이 하나의 트랜잭션에서 같이 발생되는 경우 다른 트랜잭션에 의해 변경될 가능성이 있습니다. InnoDB에서는 이러한 문제를 해결하기 위해 `FOR SHARE`, `FOR UPDATE` 라는 `Locking Reads`를 제공합니다.\n\n**`SELECT … FOR SHARE`** 는 조회하는 인덱스 레코드에 `S-Lock` 을 설정합니다. 다른 트랜잭션에서 레코드를 읽을 수는 있지만, `S-Lock`을 설정한 트랜잭션이 커밋되기 전까지 레코드를 수정할 수 없으며, `X-Lock`을 설정할 수 없습니다.\n\n**`SELECT … FOR UPDATE`** 는 조회하는 인덱스 레코드에 대해 `UPDATE` 문을 실행한 것과 동일하게 레코드 및 관련 인덱스에 `X-Lock`을 설정합니다. 다른 트랜잭션에서 레코드를 읽거나 쓰는 작업이 불가능하며, `S-Lock`이나 `X-Lock`을 설정할 수 없습니다.\n\n이러한 `Locking Reads`를 바탕으로 `Locking`을 구현하게 됩니다.\n\n### Locking과 Consistent Read\n\nInnoDB에서 기본적으로 `Locking`은 트랜잭션의 격리 수준을 구현되기 위해서 사용됩니다. 이를 구현하기 위한 대표적인 방법으로 **Record Lock, Gap Lock, Next-Key Lock**을 살펴보고자 합니다.\n\n**`Record Lock`** 은 하나의 인덱스 레코드에만 Lock을 거는 것을 말합니다. 만약 테이블에 인덱스가 정의되지 않았다고 하더라도 InnoDB를 기준으로 primary key 혹은 unique key를 통해서 Clustered Index를 생성하기 때문에 이를 활용하여 Record Lock을 적용할 수 있습니다.\n\n만약 `SELECT content FROM coupon WHERE id = 1 FOR UPDATE;` 와 같은 쿼리를 실행하게 된다면 id가 1에 해당하는 레코드에 Record Lock이 설정되기 때문에 다른 트랜잭션에서 해당 레코드를 변경할 수 없습니다.\n\n**`Gap Lock`** 은 인덱스 레코드간의 범위에 Lock을 거는 것을 말합니다. 최초 레코드 이전, 마지막 레코드 이후를 가상의 인덱스 레코드로 생각해서 Lock을 설정할 수 있으며, 다른 트랜잭션에서 해당 범위에 데이터를 삽입하거나 변경할 수 없습니다.\n\n만약 `SELECT content FROM coupon WHERE id ≥ 1 FOR UPDATE;` 와 같은 쿼리를 실행하고 다른 트랜잭션이 id가 10인 데이터를 삽입하려고 한다면, 해당 id 값은 Gap Lock으로 설정되어 있기 때문에 삽입하거나 혹은 변경할 수 없습니다.\n\n**`Next Key-Lock`** 은 Record Lock과 해당 인덱스 레코드 앞의 Gap에 대한 Gap Lock을 조합하여 Lock을 거는 것을 말합니다.\n\n이렇게 `Locking`이 설정됨에 따라서 격리 수준을 구현할 수 있게 되었지만 동시성 성능은 떨어지게 되었습니다. MySQL의 InnoDB는 이렇게 동시성 성능이 떨어지는 문제를 해결하기 위해서 `MVCC`라는 개념을 도입하였습니다. 이는 각 트랜잭션의 격리 레벨에 따라 상이하지만, 특정 시점의 `Snapshot` 정보를 바탕으로 하여, 기존과 같이 `Locking`을 사용하지 않고도 `일관된 읽기(Consistent Read)`를 제공하여 동시성을 제어할 수 있게 됩니다.\n\n## 트랜잭션 격리 레벨에 대해서 자세히 알아보자\n\n지금까지의 내용들을 바탕으로 트랜잭션 격리 레벨을 구성할 수 있으며 READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE 총 4단계로 구분할 수 있습니다.\n\n그러면 4가지 키워드들에 대해서 하나씩 살펴보도록 하겠습니다.\n\n### READ UNCOMMITTED\n\n`Read Uncommitted`는 커밋 전의 트랜잭션의 데이터 변경 내용을 다른 트랜잭션이 읽는 것을 허용하는 격리 레벨을 말합니다. `Read Uncommitted` 에서는 일반적인 SELECT 문은 Non-Locking Read로 수행되지만 MVCC를 사용하지 않아 Consistent Read를 보장하지 않습니다.\n\n`Read Uncommitted` 에서는  `Dirty Read`현상이 발생할 수 있습니다. `Transaction 1`에서 INSERT로 추가된 `Coupon`이 `COMMIT` 되기 이전에 새로운 `Transaction 2`에서 `Coupon`을 조회한다고 가정해보겠습니다. 하지만 `Transaction 1`에서 오류가 발생해서 `ROLLBACK`이 되게 되었습니다. 이 경우 `Transaction 2`는 `ROLLBACK` 여부를 확인하지 못하고 `ROLLBACK` 된 `Coupon`를 정상적인 데이터라 생각하고 작업을 계속 진행하게 됩니다.\n\n![](img.png)\n\n이처럼 트랜잭션에서 처리한 작업이 완료되지 않아도 볼 수 있는 현상을 `Dirty Read` 현상을 확인할 수 있습니다.\n\n### READ COMMITTED\n\n`Read Committed`는 `COMMIT`이 완료된 트랜잭션의 변경사항만 다른 트랜잭션에서 조회가 가능합니다. 이로 인해서 `Read Uncommitted`에서 발생하는 `Dirty Read`가 발생하지 않습니다. 이렇게 되는 이유는 `Read Committed` 부터는 `MVCC`인 `Consistent Read`로 문제를 해결하기 때문입니다.\n\n`Read Committed`에서 데이터의 변경이 일어나게 되면 변경 전 데이터는 **언두 영역으로 복사가 됩니다**. 그리고 **다른 트랜잭션에서 해당 테이블의 데이터를 조회할 경우, 변경된 테이블 데이터를 조회하는 것이 아니라 언두 영역에 복사된(Snapshot) 레코드를 조회**하게 됩니다. 이 때문에 발생할 수 있는 문제를 해결할 수 있습니다.\n\n![](img_1.png)\n\n하지만 `Read Committed`는 `COMMIT` 된 데이터에 대해서는 정합성을 유지한다는 판단을 하기 때문에 `Commit`이 발생할 경우 기존 `Snapshot`을 새로운 `Snapshot`으로 다시 덮어쓰게 됩니다. 이 때문에 `Read Committed`에서는 `None Repetable Read`현상이 발생할 수 있습니다.\n\nDB에는 `Coupon`이 2개가 있다고 가정해보겠습니다. `Transaction 1`에서 쿼리가 먼저 시작되고 나서 `Transaction 2`에서 `Coupon` 의 2번 ID를 조회하면 **커피가 조회가 되게 됩니다**. 하지만 `Transcation 1`에서 **UPDATE** 쿼리가 `COMMIT` 된 이후 `Transaction 2` 에서 다시 똑같은 **SELECT** 쿼리를 사용하여 다시 `Coupon`를 조회하게 될 경우 **UPDATE** 쿼리의 경우 **술이 조회가 되게 됩니다.**\n\n![](img_2.png)\n\n이처럼 **하나의 트랜잭션 내에서 똑같은 SELECT 쿼리를 실행했을 때 항상 같은 결과를 가져오지 못하는 `None-Repetable Read`가 발생하게 됩니다.**\n\n또한 Read Committed 레벨에서는 `Record Lock`만을 사용하기 때문에 **데이터가 중간에 삽입, 삭제 될 경우 SELECT의 결과가 다르게 나타나는 `Phantom Read` 현상도 발생**하게 됩니다.\n\n### REPEATABLE READ\n\n`Repeatable Read`는 트랜잭션 범위 내에서 조회한 내용이 항상 동일함을 보장합니다. 여기서 `Read Committed`와 다른 점은 언두 영역에 백업된 레코드의 여러 버전 가운데 몇 번째 이전의 버전을 찾는지에 있습니다. 모든 `InnoDB`의 트랜잭션은 고유한 트랜잭션 번호를 가지며, 언두 영역에 백업된 모든 레코드에는 변경을 발생시킨 트랜잭션의 번호가 포함되어 있습니다.\n\n밑의 그림과 같이 자신의 트랜잭션 번호보다 작은 트랜잭션 번호만 보게 되는 것을 확인할 수 있습니다. 이를 통해 처음 생성된 `Snapshot`을 기반으로 하나의 트랜잭션 내에서 일관된 읽기를 제공합니다.\n\n![](img_3.png)\n\n`Repeatable Read` 는 일반적인 SQL 표준에서는 `Phantom Read`가 발생하게 됩니다. 밑의 사진을 보게되면 조회를 하기 위해서 `SELECT … FOR UPDATE` 쿼리를 사용하고 있습니다. `FOR UPDATE` 쿼리를 사용하게 되면 레코드에 쓰기 잠금을 걸어버리게 되는데, 언두 레코드에는 이를 적용할 수 없습니다. 이 때문에, 언두 영역의 변경 전 데이터(Snapshot)를 가져오는 것이 아니라, 현재 레코드의 값을 가져오게 되어 `Phantom Read`가 발생한다고 할 수 있습니다.\n\n![](img_4.png)\n\n하지만 저희가 주로 사용하는 MySQL의 `InnoDB` **기준으로는 `Repetable Read` 조건에서 `Phantom Read`가 발생하지 않습니다.** 그 이유는 Locking Read와 UPDATE, DELETE 문의 경우 검색 조건에 따라 사용되는 `Lock`이 다르게 적용되기 때문입니다.\n\n- 고유한 검색 조건이 있는 고유 인덱스에 대한 쿼리는 Record Lock이 적용\n- 범위 검색 조건의 경우 스캔한 인덱스 범위에 Gap Lock과 Next-key Lock이 적용\n\n이러한 이유 때문에 중간에 특정 데이터가 추가 혹은 변경되어 발생하는 `Phantom Read` 현상은 `InnoDB`의 `Repetable Read`에서는 발생하지 않습니다.\n\n> 그럼에도 발생할 수 있는 `Phantom Read`\n>\n\n일반적으로는 거의 발생하지 않지만 `Repetable Read` 수준임에도 불구하고 `Phantom Read`가 발생할 수 있는 상황이 있습니다.\n\nB 트랜잭션이 추가한 레코드에 A 트랜잭션이 UPDATE 쿼리를 수행하게 될 경우, 처음 SELECT 쿼리로 생성된 `Snapshot` 에는 존재하지 않지만 실제 디스크에는 데이터가 존재하기 때문에 해당 레코드가 영향을 받게 됩니다. 이후 SELECT 쿼리를 수행할 때 `Snapshot`이 초기화되어 `Phantom Read`가 발생하게 됩니다.\n\n하지만 해당 상황은 조금 억지스럽게 `Phantom Read`가 발생할 수 있는 상황을 연출한 것이라 해당 상황이 생길 수 있다는 것만 인지를 하고 넘어가도록 하겠습니다.\n\n### Serializable\n\n가장 단순한 격리 수준이면서, 가장 엄걱한 격리 수준입니다. 일반적인 읽기 작업에 대해서도 `S-Lock`을 획득해야 하며, 동시에 다른 트랜잭션은 해당 레코드의 변경이 불가능 합니다. 이 때문에 한 트랜잭션에서 읽고 쓰고 있는 레코드에 대해서는 다른 트랜잭션에서 절대 접근할 수 없습니다.\n\n하지만 일반적인 읽기 작업에 대해서도 `S-Lock`을 통해 블로킹 처리를 수행하기 때문에 동시성이 매우 떨어지게 되어 사용하는 것을 권장하지 않습니다.\n\n## 스프링에서의 트랜잭션\n\n이처럼 트랜잭션은 위에서 설명한 트랜잭션의 격리 레벨을 이용하여 트랜잭션의 원칙 중 하나인 격리성을 수행한다고 할 수 있습니다. 그렇다면 스프링에서는 이러한 트랜잭션을 어떤 방식을 통해 구현하고 있을까요??\n\n### 스프링이 제공하는 트랜잭션 기술\n\nJDBC를 사용하는 개발자가 직접 여러 개의 작업을 하나의 트랜잭션으로 관리하기 위해서는 Connection 객체를 공유하는 작업을 추가적으로 해주어야 합니다. 이러한 로직을 매번 개발자가 작성해주어야 하기 때문에 상당히 불편한 작업이라고 할 수 있습니다.\n\nSpring은 이러한 문제를 해결하기 위해 트랜잭션을 동기화 하는 기술을 제공하고 있습니다. `TransactionSynchronizeManager`을 통해서 트랜잭션을 시작하기 위한 Connection 객체를 특정 저장소에 보관해 두고 필요할 때 꺼내서 사용하는 방식으로 동기화를 적용할 수 있습니다. 또한 해당 동기화는 작업 스레드마다 Connection 객체를 독립적으로 관리하기 때문에 멀티 스레드 환경에서도 문제가 발생할 여지 없이 Connection 객체를 공유할 수 있습니다.\n\n```java\nTransactionSynchronizeManager.initSynchronization();\nConnection connection = DataSourceUtils.getConnection(dataSource)\n```\n\n하지만 해당 코드는 JDBC만 사용했을 때 유효합니다. 만약 JPA로 코드를 변경해야 한다면 기존의 Connection을 획득하는 코드를 변경해야 할 것입니다.\n\nSpring은 이러한 문제를 해결하기 위해 트랜잭션을 추상화 하는 기술을 제공하고 있습니다. `PlatformTransactionManger`를 통해서 각 트랜잭션 기술들의 공통점을 담은 트랜잭션 추상화 기술을 이용해 일관되게 트랜잭션을 처리할 수 있도록 기능을 제공합니다.\n\n![](img_5.png)\n\n또한 스프링은 각 데이터 접근 기술에 대한 트랜잭션 매니저의 구현체도 제공을 하고 있습니다. 이 때문에 우리는 필요한 구현체를 스프링 빈으로 등록하고 주입받아 사용하기만 하면 트랜잭션을 사용할 수가 있습니다. 여기에 스프링 부트까지 사용한다면 `AutoConfiguration`을 통해서 의존성으로 등록한 데이터 접근 기술을 자동으로 인식해, 적절한 트랜잭션 메니저를 스프링 빈으로 등록해주어 개발자가 사용할 수 있게 됩니다.\n\n### @Transactional 등장\n\n이렇게 스프링에서 제공하는 동기화 기법인 `TransactionSynchronizeManager`와 추상화 기법인 `PlatformTransactionManager`를 통해서 조금 더 편리하게 개발이 가능해 졌습니다.\n\n하지만 아직까지 문제가 있습니다. 바로 비즈니스 코드와 데이터 엑세스 기술이 코드에 강하게 결합한다는 문제가 발생합니다.\n\n```java\n@Service\npublic class UserSerivce {\n\n    public void businessMethod() {\n        TransactionStatus status = transactionManager.getTransaction(new DefaultTransactionDefinition());\n        try {\n            businessLogic(...);\n            transactionManager.commit(status);  // 성공시 커밋\n        } catch (Exception e) {\n            transactionManger.rollback(status); // 실패시 롤백\n            throw new IllegalStateException(e);\n        }\n    }\n}\n```\n\n이는 코드의 유지보수를 어렵게 하고 중복된 코드를 만들어 개발자의 작업 생산성을 저하시킵니다. 이러한 문제를 해결하기 스프링은 **프록시 방식을 적용하여 비즈니스 코드와 데이터 엑세스 기술을 분리하여 사용할 수 있도록 기능을 제공**하고 있습니다.\n\n```java\n// 프록시 코드\n@Component\npublic class TransactionUserSerivceProxy {\n\n    private UserService target;\n    \n    public void businessMethod() {\n        TransactionStatus status = transactionManager.getTransaction(new DefaultTransactionDefinition());\n        try {\n            target.businessMethod();\n            transactionManager.commit(status);  // 성공시 커밋\n        } catch (Exception e) {\n            transactionManger.rollback(status); // 실패시 롤백\n            throw new IllegalStateException(e);\n        }\n    }\n}\n\n// 실제 비즈니스 코드\npublic class MemberService {\n\n    public void businessMethod() {\n        businessLogic(...);\n    }\n}\n```\n\n또한 **Advisor 기능**과 **동적 프록시 생성기**를 이용한 **트랜잭션 애노테이션(@Transactional)** 을 통해 개발자가 조금 더 편하게 선언적으로 트랜잭션을 이용할 수 있도록 기능을 제공하고 있습니다.\n\n```java\n@Transactional\n@Service\npublic class MemberService {\n\n    public void businessMethod() {\n        businessLogic(...);\n    }\n}\n```\n\n## 다음 글에서는..\n\n이번 글에서는 트랜잭션의 관점에서 어떻게 동시성 문제를 해결하는지 살펴보고, Spring에서는 어떻게 트랜잭션을 지원하는지 그 개념과 동작원리를 살펴보는 시간을 가졌었습니다. 다음 글에서는 실제 주어진 요구사항을 해결하기 위해 필요한 추가적인 개념과 실제 코드를 통해 해결하는 과정을 살펴보도록 하겠습니다.\n\n## 참고\n\n- [https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html](https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html)\n- [https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html](https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html)\n"},{"excerpt":"우테코 미션을 진행하다보면 유행?이 되는 논쟁이 있습니다. 그 중에 하나가 Repository와 DAO의 차이점 입니다. 당시 미션을 진행할 때는 “그냥 부르는 명칭의 차이가 아닌가?” 라는 생각을 했었습니다. 그리고 지금 이 시점에서 어느 정도 Repository와 DAO의 차이에 대해서 설명할 수 있을 것 같은데요. 이번 글에서는 제가 생각하고 있는 …","fields":{"slug":"/DAO와-Repository/"},"frontmatter":{"date":"November 01, 2022","title":"DAO와 Repository","tags":["Repository","DAO"]},"rawMarkdownBody":"\n우테코 미션을 진행하다보면 유행?이 되는 논쟁이 있습니다. 그 중에 하나가 Repository와 DAO의 차이점 입니다. 당시 미션을 진행할 때는 “그냥 부르는 명칭의 차이가 아닌가?” 라는 생각을 했었습니다. 그리고 지금 이 시점에서 어느 정도 Repository와 DAO의 차이에 대해서 설명할 수 있을 것 같은데요. 이번 글에서는 제가 생각하고 있는 Repository와 DAO의 차이점에 대해서 정리를 해보고자 합니다.\n\n## DAO란 무엇일까??\n\n`DAO`는 Data Acess Object의 약자이며, J2EE에서 최초로 등장한 개념입니다. 인터페이스를 활용한 API를 제공하여 어플리케이션 계층과 퍼시스턴스 계층을 분리하기 위해서 만들어진 패턴이라고 할 수 있습니다. **하나의 데이터베이스 테이블에 대한 저수준의 CRUD 책임을 가지기 때문에, 한 테이블 당 1 : 1로 매핑**되어서 만들어집니다.\n\n그렇다면 이 `DAO`는 왜 만들어지게 되었을까요?? 어플리케이션을 개발하다보면 영구한 저장소가 필요하게 됩니다. 흔히 저희가 데이터베이스라고 명칭하는 것들을 말하는데요. 이러한 데이터베이스는 저희가 아는 종류만 해도 매우 다양하게 존재합니다. 이러한 데이터베이스에 접근하기 위해서 각 데이터베이스에서 제공하는 Public API를 통해서 접근을 하게 됩니다.\n\n하지만 이러한 데이터베이스 API를 직접적으로 사용하게 되면 몇가지 문제가 발생할 수 있습니다.\n\n일반적으로 데이터베이스의 종류로 MySQL을 사용하게 됩니다. 이때 데이터베이스 운용 가격의 문제로 Maria DB로 변경해야 한다고 요구사항이 발생했다면, 우리는 이때까지 MySQL에서 제공하는 API로 작성된 로직을 전부 변경해야하는 문제가 발생하게 됩니다. 이는 어플리케이션 계층과 퍼시스턴스 계층에 속한 MySQL 관련 로직이 **직접적으로 강하게 연결되어 있어 변경에 취약하다는 점이 문제**라고 할 수 있습니다. 이는 객체지향적 관점에서 보더라도 `OCP`를 위반하였다고 할 수 있습니다.\n\n이러한 문제를 `DAO`를 통해서 해결할 수 있습니다. 앞서서 설명했듯이 데이터베이스 구현체를 그대로 사용하지 않고 인터페이스를 통해 API를 추상화 함으로써, **실제 구현체가 변경되더라도 어플리케이션 계층을 변경하지 않아도 되는 효과**를 가질 수가 있습니다.\n\n## Repository란 무엇일까??\n\n그렇다면 `Repository`는 무엇일까요?? **도메인 주도 설계**에서 정의한 `Repository`는 다음과 같습니다.\n\n> 전역적인 접근이 필요한 각 객체 타입에 대해 메모리상에 해당 타입의 객체로 구성된 컬렉션이 있다는 착각을 불러 일으키는 객체를 만든다. 잘 알려진 전역 인터페이스를 토대로 한 접근 방법을 마련하라. 객체를 추가하고 제거하는 메서드를 제공하고, 이 메서드가 실제로 데이터 저장소에 데이터를 삽입하고 데이터 저장소에서 제거하는 연산을 캡슐화하게 하라. 특정한 기준으로 객체를 선택하고 속성값이 특정 기준을 만족하는 완전히 인스턴스화된 객체나 객체 컬렉션을 반환하는 메서드를 제공함으로써 실제 저장소와 질의 기술을 캡슐화하라. 실질적으로 직접 접근해야 하는 AGGREGATE의 루트에 대해서만 REPOSITORY를 제공하고, 모든 객체 저장과 접근은 REPOSITORY에 위임해서 클라이언트가 모델에 집중하게 하라.\n>\n\n여기서 우리가 주목해야 할 부분은 **객체로 구성된 컬랙션**과 **캡슐화** 라고 생각합니다. 일반적인 웹 어플리케이션 아키텍쳐를 살펴보겠습니다. 여기서 `Repository`는 어느 계층에 속한다고 말할 수 있을까요??\n\n![](img.png)\n\n우리가 개발하고 있는 프로젝트에서 `Repository`는 데이터베이스에 데이터를 저장하기 위해서 사용되게 됩니다. 그러면 **퍼시스턴스(Infrastructure) 계층**에 속한다고 말할 수 있을 것 같습니다. 하지만 **도메인 주도 설계**에서는 `Repository`는 **객체의 상태를 관리하는(CRUD) 저장소이기 때문에 도메인 계층에 속한다**고 말하고 있습니다. 그렇다면 무엇 때문에 우리는 `Repository`가 퍼시스턴스 계층에 속한다고 생각했던 걸까요??\n\n우리는 대부분의 프로젝트에서 JPA를 사용하고 있습니다. 이때 JPA를 통해서 데이터베이스에 접근하기 위해서 우리는 `JpaRepository`라는 객체를 상속받아 `XXXRepository`를 구현해 사용을 하고 있습니다. 이 부분 때문에 우리는 `**Repository`가 퍼시스턴스 계층에 속한다고 생각**하게 됩니다.\n\n하지만 생각해보면 우리는 어플리케이션 계층에서 비즈니스 로직을 작성할 때 `Repository` 내부가 어떻게 구현되어 있는지 모르고 사용을 합니다. 단순히 **Public API를 통해 나와있는 그 기능을 사용할 뿐**입니다. 예를 들면, `save()` 메서드가 존재하면 해당 객체를 저장하는 기능이구나, `find()` 메서드가 있으면 해당 객체를 조회하는 기능이구나 와 같이 생각하는 것 처럼 말입니다. (물론 이론상 그렇다는거고 실제로는 JPA의 동작방식을 이해해야 한다고 개인적으로 생각합니다.)\n\n이처럼 내부 구현을 몰라도 어플리케이션 계층에서 사용이 가능한 이유는 `Repository`가 `Interface`로 구성이 되어 있기 때문입니다. 내부 구현체를 숨김으로써 **어플리케이션 계층에서는 도메인의 상태만 관리하는 기능을 수행하도록 API를 제공**하고, 실제 구현체에서는 **해당 도메인을 영속화하는 기능을 수행하기 위해 인프라 스트럭쳐와 관련되 모듈들을 Import 하여 기능을 동작**한다고 할 수 있습니다. 즉, 정리하면 우리가 사용하는 관점에서 `Repository`의 `Interface`는 도메인 레이어, `Repository`의 구현체는 퍼시스턴스(영속성) 레이어에 속한다고 할 수 있습니다.\n\n## Dao와 Repository의 차이\n\n지금까지 내용을 통해 `DAO`와 `Repository`의 핵심적인 차이점에 대해서 정리를 하면, `**DAO`는 데이터베이스의 테이블과 1 : 1 매핑이 되는 저수준의 CRUD 기능**을 제공한다고 할 수 있습니다. `Repository`는 도메인에 대해서 CRUD 기능을 제공하며 이러한 도메인은 데이터베이스 테이블과 1 : 1 매핑이 되는 것이 아닌 하나의 개념 단위를 묶인 것(Aggregate)이라고 할 수 있습니다. 즉, `Repository`의 구현체에서는 여러개의 `DAO`를 조합하는 방식으로 하나의 개념을 군집화하여 구현할 수도 있다 할 수 있습니다.\n\n```java\npublic class OrderRepositoryImpl extends OrderRepository {\n\n    private final OrderTableDao orderTableDao;\n    private final OrderDao orderDao;\n    // 생략..\n}\n```\n\n사실 `DAO`를 사용하더라도 도메인에서 사용하는 `Repository Interface`와 같이 구현하면 충분히 `Repository`와 동일하게 사용을 할 수 있다고 생각합니다. 하지만 애초에 `DAO`와 `Repository` 설계의 목적 자체가 다르다고 생각을 하기 때문에 목적에 맞게 사용을 하는 것이 좋다고 생각합니다. 어쩌면 JPA에서 `JpaRepository` 라는 이름을 통해서 `Repository`를 제공하고 있기 때문에 `Repository`르 명명되는 것들이 자연스럽게 된 것 같기도 합니다\n\n## 참고\n\n- [https://johngrib.github.io/wiki/pattern/repository/](https://johngrib.github.io/wiki/pattern/repository/)\n- [https://www.baeldung.com/java-dao-vs-repository](https://www.baeldung.com/java-dao-vs-repository)"},{"excerpt":"테스트 격리를 위해서는 데이터를 롤백시키는 과정이 필요합니다. 하지만 이 과정에서 잘못된 방법을 사용할 경우 몇가지 문제가 발생할 수 있는데요. 이번 포스팅에서는 문제가 생길 수 있는 상황들과 테스트 격리 방법을 조금 더 고도화하기 위해서 고민했던 내용에 대해서 작성해보려고 합니다. SpringBootTest는 @Transactional로 롤백되지 않을 …","fields":{"slug":"/테스트에서의 @Transactional/"},"frontmatter":{"date":"October 29, 2022","title":"테스트에서의 @Transactional","tags":["Transactional"]},"rawMarkdownBody":"\n테스트 격리를 위해서는 데이터를 롤백시키는 과정이 필요합니다. 하지만 이 과정에서 잘못된 방법을 사용할 경우 몇가지 문제가 발생할 수 있는데요. 이번 포스팅에서는 문제가 생길 수 있는 상황들과 테스트 격리 방법을 조금 더 고도화하기 위해서 고민했던 내용에 대해서 작성해보려고 합니다.\n\n## SpringBootTest는 @Transactional로 롤백되지 않을 수도 있다\n\n`@DataJpaTest`를 사용하면 영속성 계층에 대해서 편리하게 테스트를 할 수 있습니다. `@DataJpaTest` 안에 `@Transactional`이 있어서 테스트가 끝나면 트랜잭션을 롤백시키기 때문에, 각각의 테이블이 비워지면서 모든 테스트가 격리되게 됩니다.\n\n그렇다면 `@SpringBootTest`에서는 어떻게 데이터를 롤백해야 할까요?? `@SpringBootTest`를 보면 `@Transactional`이 사용되지 않고 있습니다. 그렇다면 `@Transactional`만 잘 추가해주면 롤백이 잘 수행된다고 할 수 있을까요??\n\n### SpringBootTest의 테스트 환경\n\n기본적으로 `@SpringBootTest`는 서버를 시작하지 않습니다. 그래서 `webEnvironment` 설정을 통해 테스트를 어떤 환경에서 시작할 건지 설정을 할 수 있습니다. 설정값은 다음과 같습니다.\n\n- MOCK (default) : 내장 서버가 실행되지 않는다. mock 기반 테스트를 실행\n- RANDOM_PORT : 랜덤 포트 번호로 실제 웹 서버와 함께 WebServerApplicaitonContext가 로드됨\n- DEFINE_PORT : 설정된 포트 번호로 실제 웹 서버와 함께 WebServerApplicaitonContext가 로드됨\n- NONE : SpringApplicaiton을 통해 ApplicaitonContext를 로드하지만 어떠한 웹 환경도 제공하지 않음\n\n여기서 실제 서블릿 환경을 실행시키기 위해 `RANDOM_PORT`와 `DEFINE_PORT`로 설정을 하고 `@Transactional`을 작동시키면 롤백이 되지 않는 것을 확인할 수 있습니다. 즉, 테스트들이 격리되지 않아서 전체 테스트에 실패를 하게 됩니다. 왜 이런 현상이 발생하게 되는 걸까요??\n\n![](img.png)\n\n![](img_1.png)\n\n이렇게 **서블릿 환경을 실행시키게 되면, 별도의 스레드에서 스프링 컨테이너가 실행**되게 됩니다. 이후 테스트가 종료되고 나서 이를 롤백시키기 위해서는, 하나의 트랜잭션으로 묶여야 하는데, **스프링 컨테이너가 실제로 구동되어 테스트와 다른 스레드에서 실행이 되게 되어 `@Transactional`을 적용해도 롤백이 되지 않는 것**입니다.\n\n### 테스트 환경 격리시키기\n\n결국 실제 서블릿 환경을 실행시키는 인수 테스트를 할 때는, 모든 테이블의 데이터를 삭제해서 초기화해주는 방법이 가장 깔끔한 것 같습니다. 이를 구현하기 위해서 많은 방법들이 있겠지만 저는 `TRUNCATE` 명령어를 통해서 모든 테이블의 데이터를 제거해주도록 하겠습니다.\n\n`TRUNCATE`를 활용한 테스트 격리는 다음 순으로 이루어집니다.\n\n1. 테스트가 끝나면 모든 테이블에 대한 TRUNCATE TABLE 명령어를 얻어옴\n2. 제약조건 무호화 명령어 실행\n3. 모든 TRUNCATE TABLE 명령어를 실행시킴\n4. 제약조건 재설정 명령어를 실행시킴\n\n위의 방법들을 이용해서 `TRUNCATE` 명령어를 수행하는 로직을 구현하는 것 까지는 일반적으로 할 수 있습니다. `JPA`의 `EntityManager`를 통해서 구현할 수도 있고, `JPA` 의존이 싫다고 하면 `JdbcTemplate`을 통해서 작성도 가능합니다.\n\n하지만 이를 적용하기 위해서는 구현한 클래스에 대해서 모든 테스트 클래스에 상속을 적용해줘야 합니다. 상속 자체가 나쁜건 아니지만, 꼭 필요한 시점에 다른 상속이 필요할 때도 있고, 상위 `AcceptanceTest`에 어떠한 다른 코드가 추가되어 기존 인수 테스트에 영향을 줄 수 있는 여지가 있기 때문에 상속보다는 다른 방법으로 이를 해결하고 싶었습니다.\n\n이를 해결하기 위해서 `TestExecutionListener`, `Custom Annotation`, `Environment`를 활용하면 상속을 사용하지 않고도 인수테스트 환경을 구성할 수 있습니다. `Spring`은 테스트의 실행 주기에 개입할 수 있도록 리스너 인터페이스인 `TestExecutionListener` 를 제공하고 있습니다. 이 구현체를 만들어 테스트 실행 시점에 등록한다면 특정 시점에 개입 할 수 있게 됩니다.  제공되는 각각의 메서드들은 다음과 같습니다.\n\n![](img_2.png)\n\n여기서 우리는 각각의 테스트들이 실행되기 전 `TRUNCATE`를 해주면 되기 때문에 `beforeTestMethod`를 이용해서 구현을 하면 되겠습니다.\n\n추가적으로 자바 8 이후로 등장한 `AbstractTestExecutionListener`를 사용한다면 필요한 메서드만 오버라이딩 해서 사용할 수 있습니다.\n\n![](img_3.png)\n\n여기서 저희가 작성해야 할 기능은 크게 두 가지입니다. 하나는 기존과 같이 `TRUNCATE`를 해주는 기능이고, 또 하나는`RestAssured`을 수행하기 위해서 사용할 `Port`를 등록해주는 것입니다. `Environment` 를 통해 이러한 Port를 얻어올 수 있습니다.\n\n![](img_4.png)\n\n그리고 기존과 동일하게 `TRUNCATE` 기능을 작성해주겠습니다.\n\n![](img_5.png)\n\n지금까지 만든 `TestExecutionListener`를 테스트 실행 시에 등록하기 위해서 `@TestExecutionListeners`를 사용해주면 됩니다. 같이 사용한 `mergeMode` 속성은 `Default` 로 존재하는 다른 리스너들을 대체할 것인지를 설정하는 것인데, `Default` 리스너를 함께 사용하도록 하면 됩니다.\n\n그리고 `Custom Annotation`을 만들어 각 인수테스트에서 사용하도록 하면 깔끔하게 테스트 환경을 격리시킬 수 있습니다.\n\n![](img_6.png)\n\n## AcceptanceTest가 아닌 다른 테스트에서의 @Transactional\n\n위의 인수테스트에서는 `@Transactional`을 사용하지 않고 테스트를 수행했습니다. 그렇다면 `Random Port`를 사용하지 않아도 괜찮은 `Application Layer`에서는 `@Transactional`을 사용해도 괜찮을까요?? 다음 상황을 통해 한번 살펴보도록 하겠습니다.\n\n### 운영 코드와 테스트 코드의 괴리\n\n만약 운영코드에 실수로 `@Transactional`을 작성하지 않았다고 가정해보겠습니다. 아니면 작성이 되어있다가 운영코드에서 `@Transactional`이 지워지는 상황도 발생할 수 있겠네요. 이렇게 될 경우, `LazyInitializationException` 이 발생하게 됩니다.\n\n이유는 생각보다 간단합니다. 서비스 계층에서 트랜잭션이 걸려있지 않아 `Lazy Fetching`이 불가능하기 때문입니다. `@Transactional`이 존재하지 않아 세션이 이미 종료된 상태인데 `DataBase`에 `Lazy`로 또 접근하려 하기 때문입니다.\n\n하지만 테스트는 정상적으로 작동하게 됩니다. **그 이유는 테스트 환경에서는 `@Transactional`이 작성되어 있기 때문에 세션이 유지되기 때문**입니다. 이러한 구조는 운영 코드와 테스트 코드간에 괴리를 유발하며, 추후 장애가 발생할 수 있는 요인 중 하나라고 할 수 있습니다.\n\n### 영속성 컨텍스트에서 새로운 데이터가 조회되지 않을 수 있다\n\n보통 `JPA`를 사용한 프로젝트에서 테스트 코드에 `@Transacitonal`을 붙이게 되면, 메서드 단위로 트랜잭션이 적용되게 됩니다. 이는 **테스트 대상의 메서드가 트랜잭션을 이어받게 됨을 의미**하는데요. 이 경우 **트랜잭션 커밋이 메서드 종료시점에 수행**되므로 `Repository`를 통해 객체를 조회할 때 예상과 다르게 조회가 될 수 있습니다.\n\n```kotlin\n@Transactional\n@SpringBootTest\nclass CouponService @Autowired constructor(\n    private val couponService: CouponService,\n    private val couponRepository: CouponRepository,\n    private val userRepository: UserRepository,\n    private val userHistoryRepository: UserHistoryRepository,\n) {\n    @Test\n    fun createCoupon() {\n        // given\n        couponRepository.save(Coupon(\"커피 쿠폰\"))\n        val savedUser = userRepository.save(User(\"루키\", null))\n        userHistoryRepository.save(UserHistory(savedUser, \"커피 쿠폰\"))\n        val request = CouponRequest(1L, \"커피 쿠폰\")\n\n        // when\n        couponService.findCoupon(request) \n\n        // then\n        val results = userHistoryRepository.findAll()\n        assertThat(results).hasSize(1)\n    }\n}\n```\n\n위의 코드 중, `couponService.findCoupon()`은 `userRepository.findById()`을 호출해서 `User` 객체를 가져오게 되는데, 가져온 `User` 객체는 영속성 컨텍스트에서 캐싱된 객체로서 아직 `userHistory` 컬렉션에 객체가 추가되지 않은 상태입니다. 이러한 현상은 `userHistoryRepository.save()` 가 **상위 트랜잭션을 이어받아 하나의 트랜잭션에서 수행되어 커밋이 일어나지 않았기 때문**입니다.\n\n이를 해결하는 방법은 간단합니다. `@Transactional`을 제거한다면, 새로운 트랜잭션에서 요청이 수행되면서 정상적으로 테스트가 성공하게 됩니다.\n\n## 마무리\n\n이 포스팅 주제와는 관련이 없는데, 레벨 2 쯤이였나.. 주말에 수달이랑 프롤로그 근로 때문에 인수테스트를 같이 살펴보고 있었는데 지나가던 포비가 말을 걸어주셔서 인수테스트 관련으로 잠깐 얘기한 적이 있었습니다. 당시 인수테스트의 가독성 향상과 속도 향상에 빠져있던 시기라 포비에게 이것저것 여쭤봤었고, 그 중에 한가지가 기억이 남았습니다.\n\n**‘현업에서도 인수테스트에 대해서 고민을 많이하고, 지금의 코드처럼 상속을 통해서 해결하기도 해요. 하지만 상속으로 하면 제약사항이 생기게 되잖아요?? 이 부분에 대해서 한번 고민해보는 것도 좋을 것 같아요.’**\n\n당시에 이 말을 듣고, 여려 방면으로 인수테스트에서의 상속 제거에 대해서 고민해보고 시도해봤는데 번번히 실패를 했어서 잠시 포기를 했었습니다… 그러다가 최근 구구의 JdbcTemplate 미션 때문에 검색하던 도중 `AbstractTestExecutionListener`라는 방법과 `Property`를 가져올 수 있는 방법을 알게되어서 이 문제를 해결할 수 있었습니다.\n\n아직 인수 테스트에서의 제가 고민하고 있는 부분들은 완벽하게 해결되지는 않았습니다. **테스트의 가독성이라는 고민**은 계속해서 남아있는 상황인데, 프롤로그에서 사용하고 있는 `BDD` 기반의 `Cucumber`를 사용하지 않고도 가독성있게 테스트를 작성할 수 있는 방법을 계속 고민해보고 시도해보려고 합니다.\n\n> 참고\n>\n- [https://docs.spring.io/spring-boot/docs/2.1.6.RELEASE/reference/html/boot-features-testing.html](https://docs.spring.io/spring-boot/docs/2.1.6.RELEASE/reference/html/boot-features-testing.html)\n- [https://www.baeldung.com/spring-testexecutionlistener](https://www.baeldung.com/spring-testexecutionlistener)\n- [https://github.com/next-step/atdd-subway-path](https://github.com/next-step/atdd-subway-path)\n- [https://github.com/woowacourse/jwp-refactoring/pull/334/commits/2fa9080066998b4c7e4ba36e3aab7d99433f9861](https://github.com/woowacourse/jwp-refactoring/pull/334/commits/2fa9080066998b4c7e4ba36e3aab7d99433f9861)"},{"excerpt":"들어가기 전에.. 데이터 접근 기술에 관련된 개발 효율성은 를 알기 전과 후로 나뉜다고 할 수 있습니다. 그만큼 는 개발자에게 많은 도움을 주는 도구라고 할 수 있습니다. 하지만 그만큼 잘못 사용하면 독으로 다가올 수도 있는 도구인데요. 이번 글에서는 를 사용함으로써 겪었던 문제들을 소개하기 전, 먼저 그에 대한 개념을 설명하고 각각의 문제사항에 대해서 …","fields":{"slug":"/JPA를-사용함으로서-겪었던-문제들/"},"frontmatter":{"date":"October 24, 2022","title":"JPA를 사용함으로서 겪었던 문제들","tags":["JPA"]},"rawMarkdownBody":"\n## 들어가기 전에..\n\n데이터 접근 기술에 관련된 개발 효율성은 `JPA`를 알기 전과 후로 나뉜다고 할 수 있습니다. 그만큼 `JPA`는 개발자에게 많은 도움을 주는 도구라고 할 수 있습니다. 하지만 그만큼 잘못 사용하면 독으로 다가올 수도 있는 도구인데요.\n\n이번 글에서는 `JPA`를 사용함으로써 겪었던 문제들을 소개하기 전, 먼저 그에 대한 개념을 설명하고 각각의 문제사항에 대해서 공유해보는 시간을 가져보겠습니다.\n\n## 영속성 컨텍스트\n\n영속성 컨텍스트는 `Entity`를 논리적인 개념으로 영구히 저장하는 환경을 말합니다. 이러한 영속성 컨텍스트는 **엔티티 매니저를 통해서 접근**할 수가 있습니다. 엔티티 메니저는 **하나의 쓰레드마다 하나의 엔티티 메니저가 대응(하나의 트랜잭션 단위라고 봐도 됨)**되며 내부적으로 `DB Connection Pool`을 사용해서 `DB`에 접근을 수행합니다.\n\n이렇게 관리되는 영속성 컨텍스트는 4가지 상태의 생명주기(Transient, Managed, Detached, Removed)로 분류할 수 있습니다.\n\n### 문제 상황 - 왜 SELECT 쿼리가 발생하는 걸까?\n\n예전에 프로젝트를 하던 도중, `UUID`를 `Entity`의 ID 값으로 설정해야 하는 경우가 있었습니다. 해당 `Entity`를 기반으로 비즈니스 로직을 작성하고 이에 기반한 테스트 코드도 무사히 잘 작동하는 것을 보고 안심하고 다른 작업을 수행하려고 하는데 이상한 점을 발견할 수 있었습니다.\n\n```java\nHibernate: \n    select\n        studyLog0_.id as id1_0_0_,\n        studyLog0_.name as name2_0_0_ \n    from\n        studyLog studyLog0_ \n    where\n        studyLog0_.id=?\n\nHibernate: \n    insert \n        into\n            studyLog\n            (name, id) \n        values\n            (?, ?)\n```\n\n위와 같이 `INSERT` 작업 전, `SELECT` 쿼리가 한번 발생한 후 `INSERT`가 수행되는 문제를 확인할 수 있었습니다.\n\n### Spring Data JPA에서의 save() 처리 방식\n\nSpring Data JPA의 `SimpleJpaRepository`는 `JpaRepository`를 구현하는 구현체로 제공되는 기본적인 `CRUD`가 어떻게 동작하는지 확인을 할 수 있습니다.\n\n여기서 코드를 확인해보면, 새로운 객체를 생성할 때는 `persist()`, 객체를 수정할 때는 `merge()`를 호출하여 인스턴스의 영속성 관리를 해주고 있는 것을 확인할 수 있습니다.\n\n![](img.png)\n\n코드를 보면 `entityInformation.isNew()` 라는 메서드가 존재합니다. isNew() 메서드는 새로운 Entity를 판단하기 위해서 ID 값을 확인하는데 기본 전략을 다음과 같습니다.\n\n- ID 타입이 객체 타입일 때 : null\n- ID 타입이 기본 타입일 때 : 0\n\n이를 통해서 `Entity`를 식별하는 값인 ID 값의 존재 여부를 파악한 뒤, `persist`와 `merge`를 선택해 수행한다고 할 수 있습니다.\n\n여기서 문제가 되는 점은 바로 `entityInformation.isNew()` 부분이였습니다. 일반적인 경우 `@GeneratedValue(strategy = *IDENTITY*)`를 사용하기 때문에 ID 값이 `null` 이여서 `persist` 가 수행이 되었습니다. 하지만 지금과 같이 ID 값이 이미 존재하기 때문에 `merge`가 수행된 것이였습니다.\n\n`merge`는 영속성 컨텍스트의 1차 캐시에 해당 식별자가 있는지 확인하고 없는 경우 `database`를 조회하는 작업을 수행합니다. 즉, 해당 경우에는 `UUID`이기 때문에 1차 캐시에 동일한 값이 없으므로 **무조건적으로 `SELECT` 쿼리가 한번 발생하게 되는 구조**라고 할 수 있습니다.\n\n### 문제 해결 방법\n\n다행이도 `JPA`에서는 `Persistable` 인터페이스를 재정의해서 문제를 해결할 수 있습니다.\n\n![](img_1.png)\n\n`isNew()` 메서드를 재정의해서 새로운 엔티티 확인 여부를 개발자가 직접 정의할 수 있도록 구현을 할 수 있습니다.\n\n```java\n@Getter\n@NoArgsConstructor(access = AccessLevel.PROTECTED)\npublic class StudyLog implements Persistable<String> {\n    @Id\n    private String uuid;\n\n    @CreatedDate\n    private LocalDateTime createdDate; \n\n    public Item(String id) {\n        this.id = id;\n    }\n\n    @Override\n    public boolean isNew() {\n        return createdDate == null; \n    }\n}\n```\n\n이렇게 구현을 하게 될 경우 원하는 대로 `INSERT` 쿼리가 한번만 나가는 것을 확인할 수 있습니다.\n\n```java\nHibernate: \n    insert \n        into\n            studyLog\n            (name, id) \n        values\n            (?, ?)\n```\n\n## Flush\n\n`entityManager.commit()`을 호출하게 되면 JPA의 영속성 컨텍스트에 있는 객체들이 DB로 반영되게 됩니다. 하지만 실제로는 `commit()` 메서드가 `flush()` 메서드를 호출해 이를 반영하는 것입니다.\n\n`Flush`는 영속성 컨텍스트의 내용을 `DB`에 반영하는 것을 말하며, 정확하게는 `쓰기 지연 SQL` 저장소에 있는 `SQL` 쿼리가 `DB`로 보내주는 역할을 한다고 할 수 있습니다.\n\n`Flush`가 호출되는 상황은 크게 다음과 같습니다.\n\n- 트랜잭션에 commit이 발생할 때\n- EntityManager의 flush 메서드를 호출했을 때\n- JPQL 쿼리가 실행될 때 - (정확하게 말하면 틀린말)\n\n### 문제 상황 - 왜 데이터베이스에 반영이 안되지..?\n\n변경감지를 통해서 해당 객체의 데이터 값을 변경해야 하는 비즈니스 로직을 작성해야 했었습니다. 다음과 같이 테스트 코드를 작성하고 비즈니스 로직을 완성하였습니다. (예시를 위해 코드를 간단하게 변경하였습니다)\n\n![](img_2.png)\n\n하지만 테스트가 실패하는 것을 확인할 수 있었습니다.\n\n![](img_3.png)\n\n그런데 또 웃긴상황인건 실제 프로덕션 코드에서는 정상적으로 로직이 동작하는 것을 확인할 수 있었습니다. 그렇다는 것은 실제 프로덕션 코드는 문제가 없고 테스트 코드를 잘못 작성했다는 얘기였습니다. 하지만 아무리봐도 코드 자체는 문제가 없어보였습니다.\n\n### Flush의 동작 방식을 다시 학습하자 ㅠㅠ\n\n`Flush` 호출 상황에 대한 검색을 해보면 위에서 언급된 다음 3가지 상황이 나오게 됩니다.\n\n- 트랜잭션에 commit이 발생할 때\n- EntityManager의 flush 메서드를 호출했을 때\n- JPQL 쿼리가 실행될 때\n\n그렇다면 테스트 코드에서는 이러한 조건을 하나라도 만족하는 것이 있을까요?? 눈치 채셨겠지만 해당 조건을 만족하는 코드가 존재하지 않습니다. 실제 프로덕션 처럼 `commit`을 하는 과정이 없기 때문입니다.\n\n### 해결 방법\n\n해결방법은 간단했습니다. 아까 말한 것 처럼, 실제 프로덕션에서는 `Transaction`이 끝나는 시점에 `commit`이 이루어지기 때문에 `Flush`가 호출되었습니다. 테스트 코드는 이러한 과정이 없기 때문에 직접 `Flush`를 날려주는 코드를 작성해주면 됩니다.\n\n![](img_4.png)\n\n### 문제 상황 - 왜 데이터베이스에 반영이 안되지 22..?\n\n프로젝트 중, 한 메서드에서 도메인의 카운트를 감소하고 다른 도메인에서 논리적 삭제를 해주는 코드를 작성해야 했었습니다. 역시 다음과 같이 테스트 코드를 작성하기 비즈니스 로직을 완성하였습니다. (예시를 위해 코드를 간단하게 변경하였습니다)\n\n![](img_5.png)\n\n`JPQL` 쿼리가 실행되기 때문에, 당연히 `Flush`가 발생할 것이라 생각했지만, 로그를 확인해보니 예상과 다르게 `Flush`가 동작하지 않는 것을 확인할 수 있었습니다.\n\n### JPQL은 해당 쿼리와 관련이 있는 엔티티만 플러시를 한다\n\n`Flush` 호출 상황에 대한 블로그 검색을 해보면 대부분 `JPQL` 쿼리가 실행될 때 `Flush`가 호출되어 있다고 작성되어 있습니다. 하지만 `Hibernate Docs`를 살펴보면 다음과 같이 작성되어 있습니다.\n\n![](img_6.png)\n\n현재 위의 코드에서는 영속성 컨텍스트에서 `Member - StudyLog`간의 `Entity Actions`가 존재하지 않습니다. 즉, 위에서 말한 `Flush`가 호출되는 상황에서  `JPQL` **쿼리가 실행될 때 Flush가 호출된다** 라는 말은 엄밀히 말하면 잘못된 말이라고 할 수 있습니다.\n\n### 해결 방법\n\n해결 방법은 간단합니다. 다음과 같이 영속성 컨텍스트에 `Flush` 처리를 해주면 됩니다.\n\n![](img_7.png)\n\n하지만 과연 프로덕션 코드에서 이렇게 `Flush`를 직접 호출하는 것이 올바른 방법인지에 대해서는 아직 의문점을 가지고 있는 상태입니다. 더 좋은 방법이 있다면 그 방법을 통해서 문제를 해결할 것 같습니다.\n\n해당 문제를 해결하면서 들었던 생각은, 가끔 간접참조를 통해서 각 `Entity`들을 설계하는 경우가 많은데 해당 경우에 **영속성 컨텍스트의 상태가 어떻게 관리가 될 것**인지 항상 고려하고 사용해야 할 것 같습니다.\n\n## N + 1\n\n`N + 1` 은 **조회 시 1개의 쿼리를 생각하고 설계를 하였으나 생각하지 못한 조회의 쿼리가 N개가 추가적으로 발생하는 문제**를 말합니다. JPA의 경우 객체에 대해서 조회를 하게 되면서, 다양한 연관관계들의 매핑에 의해 관계가 맺어진 다른 객체가 함께 조회되는 경우 `N + 1` 이 발생하게 됩니다.\n\n### 문제상황 - 즉시 로딩에서의 N + 1 문제점\n\n1 : N 관계를 가지고 있는 엔티티 관계가 있다고 가정하겠습니다. 1에 해당하는 엔티티에 대해서 전체 조회를 위해 Spring Data JPA의 `findAll()` 메서드를 이용해서 조회를 하게 되면 N + 1 문제가 발생하게 됩니다.\n\n### 작성된 JPQL은 결과를 반환할 때 연관관계까지 고려하지 않는다\n\n위처럼 `findAll()` 쿼리 메서드를 호출할 경우 `select x from Table x` 라는 `JPQL`로 번역이 되게 됩니다. `JPQL`은 결과를 반환할 때 해당 엔티티의 연관관계를 고려하지 않습니다. 즉, SELECT 절에 지정된 엔티티만 조회를 하게 됩니다.\n\n이렇게 엔티티 조회를 한 후, JPA는 조회한 엔티티의 내부 연관관계를 확인합니다. 현재 연관관계가 즉시로딩으로 설정되어 있기 때문에 연관된 엔티티를 검색하면서 N번의 쿼리가 발생하게 됩니다.\n\n![](img_8.png)\n\n![](img_9.png)\n\n### 문제상황 - 지연 로딩에서의 N + 1 문제점\n\n역시 동일하게 1 : N 관계를 가지고 있는 엔티티 관계가 있다고 가정하겠습니다. 지연 로딩으로 설정하였기 때문에 연관된 객체를 사용하는 시점에 조회를 하게 됩니다. 이 때문에 N + 1이 발생하지 않는다고 생각할 수 있지만, 상황에 따라서 N + 1 문제가 발생하게 됩니다.\n\n### 지연 로딩은 연관된 객체를 프록시로 가진다.\n\n지연 로딩을 사용한 상태에서 `findAll()` 쿼리 메서드를 호출할 경우 역시 동일하게 `select x from Table x` 라는 `JPQL`로 번역이 되게 됩니다. 이 때 SELECT 절에 지정된 엔티티만 조회하여 정보를 가져오게 되고, 연관관계가 맺어진 엔티티는 프록시로 가져오게 됩니다. 이 때문에 처음에는 N + 1 이 발생하지 않지만, 추후에 연관관계가 맺어진 엔티티를 조회한다면 프록시가 아닌 실제 엔티티를 가져오기 위해 조회쿼리가 발생해 N + 1 이 발생하게 됩니다.\n\n![](img_10.png)\n\n![](img_11.png)\n\n### 해결 방법\n\nN + 1 문제를 해결하기 위해서는 `FetchJoin`, `BatchSize`, `EntityGraph`총 3가지 방법을 통해서 문제를 해결할 수 있습니다.\n\n저는 `FetchJoin`과 `BatchSize`를 각 상황에 맞게 적절하게 섞어서 사용하기로 하였습니다. `EntityGraph`를 고려하지 않은 이유는 `Join` 방식이 `Left Outer Join`으로 강제된다는 점 때문에 좀 더 상황에 맞게 대처가 가능한 나머지 방법들을 사용하여서 문제를 해결하였습니다.\n\n> FetchJoin을 통한 해결방법\n>\n\nFetchJoin은 SQL에서 사용하는 조인의 종류가 아니고 JPQL에서 성능 최적화를 위해 제공하는 기능입니다. FetchJoin을 통해 연관된 엔티티나 컬렉션을 한 번에 같이 조회를 할 수 있습니다.\n\n```java\n@Query(\"select distinct u from User u left join fetch u.articles\")\nList<User> findFetchAll();\n```\n\n![](img_12.png)\n\n> BatchSize를 통한 해결방법\n>\n\n하이버네이트가 제공하는 `org.hibernate.annotations.BatchSize` 어노테이션을 이용하면 연관된 엔티티를 조회할 때 지정된 사이즈 만큼 SQL의 IN절을 사용해서 조회합니다. 즉, 연관관계가 있는 엔티티를 조회하면 IN 절을 통해 한번에 조회하게 되어 총 1 + 1의 쿼리가 발생하게 됩니다.\n\n![](img_13.png)\n\n![](img_14.png)\n\n### 추가적인 문제 상황 - 일대다 관계에서의 페이지네이션\n\n1 : N 관계에서의 N + 1 문제 해결과 동시에 페이지네이션까지 고려해야 한다면 `FetchJoin`이 불가능합니다.\n\nRDMS 관점에서 보면 일대다 관계는 `Collection Join`이 되기 때문에 드라이빙 테이블을 기준으로 드리븐 테이블의 값들이 중복되게 됩니다. 이렇게 조회 값들이 중복된 상태에서  `Collection Fetch`를 해서 `Paging`을 하긴했지만 `applying in memory` 즉, 인메모리를 사용해서 조인을 하는 방법을 사용하기 때문입니다. **이 방법은 `OOM`이 발생할 확률이 매우 높아서 사용을 하면 안됩니다**.\n\n그렇기 때문에 페이징을 적용해야 하는 경우 `BatchSize`로 해결하는 방법이 사실상 가장 좋은 해결책입니다.\n\n\n## 마무리\n\n지금까지 제가 `JPA`를 사용하면서 경험했던 트러블 내용들을 한번 정리해보았습니다. `JPA`는 정말 편리하게 개발을 도와주어 개발 능률을 올려주는 도구이기도 하지만 잘못된 사용시 원인 파악이 힘들어 오히려 능률을 떨어트리는 양날의 검같은 도구인 것 같다는 생각이 사용하면 할수록 드는 것 같습니다. 앞으로도 `JPA`를 항상 조심해서 사용해야겠다는 생각을 하면서 글을 마치겠습니다.\n\n> 참고한 곳\n>\n- [https://docs.jboss.org/hibernate/orm/5.4/userguide/html_single/Hibernate_User_Guide.htm](https://docs.jboss.org/hibernate/orm/5.4/userguide/html_single/Hibernate_User_Guide.html)"},{"excerpt":"문제 상황 5차 데모를 앞두고 프론트 팀의 요구사항으로 다음과 같은 요구사항을 확인할 수 있었습니다.  프론트 팀에서는 최적화 대상 중에 하나로  에 압축을 하기로 하였고, 이를 위해서 으로 압축을 하기로 결정을 하였습니다.  일반적으로 보통 에서 으로 압축을 처리할 수 있기 때문에, 압축 설정을 해주면 다음과 같이 압축이 된 것을 확인할 수 있습니다. …","fields":{"slug":"/Nginx와-Gzip을-활용한-프론트-최적화/"},"frontmatter":{"date":"October 16, 2022","title":"Nginx와 Gzip을 활용한 프론트 최적화","tags":["Nginx"]},"rawMarkdownBody":"\n## 문제 상황\n\n5차 데모를 앞두고 프론트 팀의 요구사항으로 다음과 같은 요구사항을 확인할 수 있었습니다.\n\n![](img.png)\n\n프론트 팀에서는 최적화 대상 중에 하나로 `bundle.js` 에 압축을 하기로 하였고, 이를 위해서 `gzip`으로 압축을 하기로 결정을 하였습니다.\n\n![](img_1.png)\n\n일반적으로 보통 `Nginx`에서 `gzip`으로 압축을 처리할 수 있기 때문에, 압축 설정을 해주면 다음과 같이 압축이 된 것을 확인할 수 있습니다.\n\n![](img_2.png)\n\n하지만 저희 프론트 팀은 내부적으로 압축을 최적화해서 `build` 파일을 보내주기 때문에 압축된 파일을 사용하도록 `Nginx`를 설정해야 했습니다.\n\n이를 사용하기 위해 `Nginx Docs`를 살펴보던 도중, 다음과 같은 내용을 확인할 수 있었습니다.\n\n![](img_3.png)\n\n내용을 간단하게 정리하자면, 압축된 파일 버전을 사용하지 않고 일반 파일 버전을 사용하고 싶은 경우 `gzip_static` 설정을 `on` 으로 변경해주면 된다는 내용이였습니다. 즉, 저희는 내부적으로 이미 최적화된 파일을 보내기 때문에 **일반 파일 버전을 보내주는 해당 설정을 적용하면 저희가 원하는 결과**를 얻을 수 있을 것이라 생각했습니다.\n\n![](img_4.png)\n\n하지만 불행하게도 결과는 동일했습니다. 처음에는 설정 파일을 잘못 작성해서 그런것이라 생각해 `Nginx` 설정을 지우고 다시 작성하기를 반복해 보았지만, 기존과 동일한 결과를 보여주었습니다. 이때 무언가 잘못됨을 깨닫고 공식문서를 꼼꼼하게 읽어보니 마지막 문장을 발견할 수 있었습니다.\n\n![](img_5.png)\n\n해당 설정은 **기본적으로 `Nginx` 오픈소스 빌드에 포함되지 않을 수도 있으며 별도의 모듈에 정의되어 있다는 내용**이였습니다. 결국 일반적인 오픈소스로는 이를 사용할 수 없었고, 수동으로 `Nginx`를 설치하고 필요한 모듈을 붙여주기로 결론을 내렸습니다.\n\n## 우당탕탕 Nginx 컴파일 설치\n\n### 컴파일 환경 만들기\n\n먼저 `Nginx`의 코어 및 모듈들의 동작에 필요한 라이브러리와 기본 패키지에서 제공하는 모듈을 사용할 수 있게 개발자 패키지와 컴파일러가 필요했습니다. 다음 명령어를 통해 이를 수행할 수 있습니다.\n\n```bash\napt-get install make libperl-dev libpcre3 libpcre3-dev zlib1g zlib1g-dev openssl libssl-dev libxml2-dev libxslt1-dev libgd-dev libgeoip-dev google-perftools libgoogle-perftools-dev gcc g++\n```\n\n그리고 `Nginx` 사이트에서 `Nginx`파일들을 다운 받습니다.\n\n```bash\nwget http://nginx.org/download/nginx-1.22.0.tar.gz\ntar -zxf nginx-1.22.0.tar.gz\ncd nginx-1.22.0\n```\n\n![](img_6.png)\n\n`Nginx`까지 다운을 받았다면, `Nginx`의 환경설정을 진행해줍니다. 기본 패키지에서 제공하는 모든 기능과 모듈, 실행 파일 위치와 각 기능에 필요한 디렉토리를 설정하는 것을 포함하도록 설정 파일을 작성합니다. **여기서 유의해야 할 점은 `Nginx`는 새로운 모듈을 추가해주기 위해서 재컴파일**을 해야 합니다. 저는 향후 번거롭지 않도록 패키지에 포함된 모든 모듈을 사용할 수 있도록 작성을 합니다.\n\n```bash\n./configure \\\n--sbin-path=/usr/local/sbin/nginx \\\n--conf-path=/usr/local/nginx/nginx.conf \\\n--pid-path=/var/run/nginx.pid \\\n--with-http_ssl_module \\\n--with-http_v2_module \\\n--with-http_realip_module \\\n--with-http_addition_module \\\n--with-http_xslt_module \\\n--with-http_image_filter_module \\\n--with-http_geoip_module \\\n--with-http_sub_module \\\n--with-http_dav_module \\\n--with-http_flv_module \\\n--with-http_mp4_module \\\n--with-http_gunzip_module \\\n--with-http_gzip_static_module \\\n--with-http_auth_request_module \\\n--with-http_random_index_module \\\n--with-http_secure_link_module \\\n--with-http_slice_module \\\n--with-http_degradation_module \\\n--with-http_stub_status_module \\\n--with-http_perl_module \\\n--with-mail \\\n--with-mail_ssl_module \\\n--with-stream \\\n--with-stream_ssl_module \\\n--with-google_perftools_module \\\n--with-cpp_test_module \\\n--with-debug\n```\n\n보통 저희가 사용하는 `apt` 혹은 `apt-get`을 사용해 `Nginx`를 설치하게 되면, 설정 파일이 위치하는 디렉토리가 `/etc/nginx`에 위치하게 됩니다. 하지만 수동으로 설치를 해주게 되면 `/usr/local/nginx`에서 위치하게 됩니다.\n\n**(만약 `Let’s Encrypt`를 사용한다면 해당 설정은 조금 문제가 있기 때문에, 밑의 트러블 슈팅을 참고하시면 됩니다!!)**\n\n해당 설정 입력 후 실행을 하게 되면, 특별한 문제가 없을 경우 설정 관련 옵션들이 추가되며 컴파일을 할 준비가 끝나게 됩니다.\n\n### Nginx 컴파일하기\n\n이제 `make` 명령어를 통해서 컴파일을 시작합니다.\n\n```bash\nsudo make\n```\n\n역시 컴파일도 문제가 없다면 다음과 같은 화면을 만날 수 있습니다.\n\n![](img_7.png)\n\n컴파일도 문제가 없다면 이제 설치를 진행합니다.\n\n```bash\nsudo make install\n```\n\n설치에 문제가 없다면, 이제 `Nginx`를 실행할 수 있습니다. 아까의 환경 설정에서 지정한 설정 파일 경로로 이동한 후,  `nginx.conf`에서 원하는 대로 `Nginx`를 설정한 다음 서비스를 시작하면 `Nginx`를 구동할 수 있습니다.\n\n```bash\n# Nginx 폴더 위치 찾기\nfind . -name \"*nginx*\"\n\n# Nginx 설정 파일인 nginx.conf로 이동 (sudo vim을 활용해 설정 수정)\ncd /usr/local/nginx\n\n# Nginx 서비스 시작\n/usr/local/sbin/nginx\n```\n\n![](img_8.png)\n\n그리고 압축하고자 하는 `bundle.js`가 잘 전달되었는지 확인을 해보면…\n\n![](img_9.png)\n\n드디어!! 정상적으로 압축된 파일이 넘어온 것을 확인할 수 있습니다!!\n\n> 참고. Nginx 서비스 명령어\n>\n\n```bash\n# Nginx 서비스 시작\nsudo /usr/local/sbin/nginx \n\n# Nginx 서비스 정지\nsudo /usr/local/sbin/nginx -s stop \n\n# Nginx 서비스 재시작\nsudo /usr/local/sbin/nginx -s reload \n\n# Nginx 서비스 설정 파일 syntax 테스트\nsudo /usr/local/sbin/nginx -t\n```\n\n## 트러블 슈팅하기\n\n압축 파일 전송까지 잘 되었기 때문에 모든 기능이 정상적으로 잘 동작할 것이라 생각했지만, 아쉽게도 그러지 못했습니다.\n\n### URL로 직접 접근할 경우 발생하는 404\n\n처음으로 마주한 문제는 특정 페이지로 이동할 경우 404가 발생하는 상황이 있었습니다.\n\n![](img_10.png)\n\n404가 발생한다는 것은, 라우팅 경로가 문제가 있다는 의미이므로 `location` 설정을 수정해주어야 했습니다. 그래서 다음과 같이 해당 경로의 파일을 찾지 못할 경우 최상위 `html`인 `/index.html`로 이동하는 설정을 추가해주면서 404 문제를 해결할 수 있었습니다.\n\n![](img_11.png)\n\n### HTTPS 미적용\n\n404 까지 해결되었기 때문에 더 이상의 문제는 없을 것이라 생각했으나, 다음과 같이 `HTTPS`가 적용되지 않은 것을 확인할 수 있었습니다.\n\n![](img_12.png)\n\n분명 설정 파일에는 `HTTPS`를 위한 설정이 다 되었기 때문에 적용이 되어야 했지만, 다음 화면처럼 적용이 되지 않은 것을 확인할 수 있었습니다. 몇번의 검색끝에 그 이유를 알 수 있었는데, 원인은 `nginx.conf`가 저장되는 위치가 다르기 때문이였습니다.\n\n위에서 설명했듯이 기존처럼 `apt`, `apt-get`을 통해서 `Nginx`를 설치하게 되면 `/etc/nginx` 에 위치하게 되지만, 수동으로 설치하면 `/usr/local/nginx` 에 위치하게 됩니다. 여기서 문제는 저희가 `HTTPS` 적용을 위해서 사용하는 `Let’s Encrypt`가 `/etc/nginx` 를 기준으로 설정이 적용된다는 점입니다.\n\n결국 기존과 동일하게 적용을 해주기 위해서 다음과 같이 `nginx.conf`의 설정 위치를 변경해주는 환경 설정 옵션을 작성하고, 기존과 동일하게 설치 작업을 다시 진행주었습니다.\n\n```bash\n# 기존의 nginx.conf 위치\n--conf-path=/usr/local/nginx/nginx.conf \\\n\n# 변경할 nginx.conf 위치\n--prefix=/usr/share/nginx \\\n--conf-path=/etc/nginx/nginx.conf \\\n--error-log-path=/var/log/error.log \\\n--http-log-path=/var/log/access.log \\\n```\n\n```bash\n# 변경된 설정\n./configure \\\n--sbin-path=/usr/local/sbin/nginx \\\n--prefix=/usr/share/nginx \\\n--conf-path=/etc/nginx/nginx.conf \\\n--error-log-path=/var/log/error.log \\\n--http-log-path=/var/log/access.log \\\n--pid-path=/var/run/nginx.pid \\\n--with-http_ssl_module \\\n--with-http_v2_module \\\n--with-http_realip_module \\\n--with-http_addition_module \\\n--with-http_xslt_module \\\n--with-http_image_filter_module \\\n--with-http_geoip_module \\\n--with-http_sub_module \\\n--with-http_dav_module \\\n--with-http_flv_module \\\n--with-http_mp4_module \\\n--with-http_gunzip_module \\\n--with-http_gzip_static_module \\\n--with-http_auth_request_module \\\n--with-http_random_index_module \\\n--with-http_secure_link_module \\\n--with-http_slice_module \\\n--with-http_degradation_module \\\n--with-http_stub_status_module \\\n--with-http_perl_module \\\n--with-mail \\\n--with-mail_ssl_module \\\n--with-stream \\\n--with-stream_ssl_module \\\n--with-google_perftools_module \\\n--with-cpp_test_module \\\n--with-debug\n```\n\n여기까지 완료한다면 기존과 동일하게 서비스를 적용할 수 있으며 `gzip` 까지 깔끔하게 적용된 것을 확인할 수 있습니다.\n\n![](img_13.png)\n\n> 참고자료\n>\n\n[https://docs.nginx.com/nginx/admin-guide/web-server/compression/](https://docs.nginx.com/nginx/admin-guide/web-server/compression/)"},{"excerpt":"사용배경 현재 진행하고 있는 프로젝트에서 쿠폰 시스템을 만들고 있습니다. 현재 시스템상 내부 인원들만 사용을 하고 있기 때문에 부하에 대한 문제를 걱정하지 않아도 되지만, 해당 시스템이 실제 상용서비스가 되어 많은 사용자들이 사용한다고 가정한다면 분명 서버와 DB에 많은 부하가 오게 될 것입니다. 이번 글에서는 부하 분산 중 DB에서의 처리 방법을 한번 …","fields":{"slug":"/Replication과-SpringBoot/"},"frontmatter":{"date":"October 16, 2022","title":"Replication과 Spring Boot","tags":["Replication"]},"rawMarkdownBody":"\n## 사용배경\n\n현재 진행하고 있는 프로젝트에서 쿠폰 시스템을 만들고 있습니다. 현재 시스템상 내부 인원들만 사용을 하고 있기 때문에 부하에 대한 문제를 걱정하지 않아도 되지만, 해당 시스템이 실제 상용서비스가 되어 많은 사용자들이 사용한다고 가정한다면 분명 서버와 DB에 많은 부하가 오게 될 것입니다.\n\n이번 글에서는 **부하 분산 중 DB에서의 처리 방법을 한번 고민**해보고자 합니다. 그리고 고민의 결과와 적용한 과정에 대해서도 함께 작성해보려고 합니다.\n\n## DB 성능 확장 방법\n\nDB의 성능을 높이기 위해서는 `Scale up` 또는 `Scale out` 하여 성능을 높여야 합니다. 현재 진행하는 프로젝트에서는 `Scale up`을 하기 위한 자원을 제공받을 수 없는 상황이기 때문에, `Scale up`은 고려하지 않고 `Scale out`을 적용하기로 결정했습니다.\n\nDB를 `Scale out`하는 방법은 `Clusterning`과 `Replication`으로 구분할 수 있습니다. 한번 각각의 방법에 대해서 알아보도록 합시다.\n\n### Clustering\n\n`Clusterning`은 DB 서버를 2대 이상, DB 스토지리를 1대로 구성하는 형태입니다. **DB 서버 2대를 모두 `Active` 상태로 운영한다면, DB 서버 1대가 죽더라도 다른 DB 서버 1대는 살아있어 서비스를 정상적**으로 할 수 있습니다. 또한 DB 서버를 여러대로 두게되면, 트래픽을 분산하여 감당하게 되어 CPU와 Memory 부하가 적어지는 장점이 존재합니다.\n\n![](img.png)\n\n하지만, **DB 서버들이 하나의 DB 스토리지를 공유하기 때문에 DB 스토리지에 병목이 생기는 단점**이 존재합니다.\n\n이를 해결하기 위해서 DB 서대 2대 중 1대를 `Stand-by` 즉, 사용대기 상태로 두어 단점을 보안할 수 있습니다. **`Stand-by` 상태의 서버는 `Active` 상태의 서버에 문제가 생겼을 때, `Fail Over`를 하여 상호 전환을 하여 장애에 대응**을 할 수 있습니다. 이를 통해서, DB 스토리지에 대한 병목 현상도 해결할 수 있습니다.\n\n![](img_1.png)\n\n하지만, **`Fail Over`가 발생하는 시간 동안은 손실이 존재하며, DB 서버 비용은 그대로인데 가용률이 1/2가 된다는 단점이 존재**합니다.\n\n또한, `Clustering`이 가지는 본질적인 **문제로 DB 스토리지에 문제가 발생하면, 데이터를 복구할 수 없다는 치명적인 문제**가 존재합니다.\n\n### Replication\n\n`Replication`은 각 DB 서버가 각자 DB 스토리지를 가지고 있는 형태입니다. 그리고 이를 실시간으로 동기화하면서 트래픽을 여러 MySQL 서버로 분산시켜주기 때문에 데이터베이스로 인한 병목 현상을 개선할 수 있습니다.\n\n![](img_2.png)\n\n`Master`와 `Slave`로 구성된 구조는 `Master` 서버에는 INSERT, UPDATE, DELETE 작업이 전달되고 `Slave` 서버에는 SELECT 작업을 전달합니다. `Slave`는 결국 `Master` 서버에서 복제된 데이터이기 때문에 데이터의 조작이 발생할 수 있는 INSERT, UPDATE, DELETE 작업은 `Master`로 전달이 되고 조회만을 하는 SELECT 작업은 `Slave` 서버를 통하여 진행하게 됩니다.\n\n위의 그림에서는 `Slave` 서버가 하나뿐이지만 서비스에 맞게 `Slave` 서버를 여러 개로 설정할 수도 있습니다. **데이터베이스에서 발생하는 대부분의 쿼리는 조회인 SELECT인데 이러한 것을 `Slave` 서버를 통해 분산하여 처리할 수 있으니 좀 더 성능 향상을 가져갈 수 있습니다**.\n\nMySQL에서의 `Replication` 동작과정을 조금 더 자세히 살펴보면 다음과 같습니다.\n\n1. 사용자가 INSERT, UPDATE, DELETE 쿼리를 `Master` 서버로 요청하면, `Master` 서버는 이를 처리하고 `Master` 서버에서 일어나는 모든 변경 이력들을 **바이너리 로그 스레드**를 사용해서 **바이너리 로그**에 저장한다.\n2. `Slave` 서버에서 변경 내역을 요청하면 `Master` 서버의 바이너리 **로그 덤프 스레드**가 바이너리 로그들을 읽어서 `Slave` 서버로 데이터를 전달한다.\n3.  `Slave` 서버의 **Slave I/O 스레드**가 요청한 변경 내역들을 `Slave` 서버의 **릴레이 로그**에 저장합니다.\n4. 최종적으로 `Slave` 서버의 **Slave SQL 스레드**가 **릴레이 로그**에 기록된 데이터들을 읽어서 `Slave` 서버에 적용합니다.\n\n> 바이너리 로그란?\n>\n\n`Master` 노드에서의 `DDL` or `DML` 가운데 데이터의 구조나 내용을 변경하는 모든 쿼리 문장과 이력을 기록하는 논리적인 로그를 말합니다.\n\n전체적인 흐름은 다음 도식과 같습니다.\n\n![](img_3.png)\n\n### 그래서.. 어떤 선택을?\n\n결론적으로 `DB Scale out`을 하는 방법으로 `Replication`을 선택하기로 했습니다. 이렇게 구성하게 될 경우 DB 스토리지를 여러개로 두기 때문에, 데이터 복구를 못하는 상황을 어느정도 방지할 수 있으며, `Clustering`과 다르게 DB 가용률을 최대로 가져갈 수 있다는 장점을 가져갈 수 있다고 생각했기 때문입니다.\n\n여기서 고민인 점은 `Slave DB`를 몇대로 설정하는가 입니다. 현재로써는 서비스 사용자가 많지 않기 때문에 `Master 1`, `Slave 1` 구조로 선택해도 문제가 없습니다. 하지만 현재 진행하는 설계는 다수의 사용자를 고려하여 진행하고 있기 때문에 추후 확장에 용이하도록 **N개의 Slave DB를 기준으로 설계**를 하기로 하였습니다.\n\n![](img_4.png)\n\n실제 프로젝트에는 `Master 1`, `Slave 2` 로 구성하여 작업을 진행하였습니다.\n\n## MySQL 설정\n\n### Master, Slave DB Server 사전 작업\n\nMySQL은 기본적으로 `127.0.0.1` 즉, 로컬 호스트에서만 접속할 수 있는데 아래와 같이 MySQL 설정을 변경하여 외부에서 접속이 가능하도록 변경을 해야합니다. 다음과 같이 변경을 하면 됩니다.\n\n```bash\nvim /etc/mysql/mysql.conf.d/mysqld.cnf\n\nbind-address           = 0.0.0.0\nmysqlx-bind-address    = 0.0.0.0\n```\n\n### Master DB Server 작업\n\n기본적인 데이터베이스 부터 한번 생성해보도록 하겠습니다. 먼저 `Master DB`에 대한 데이터베이스를 생성합니다.\n\n```java\nCREATE DATABASE test;\n```\n\n테스트할 테이블이 있어야 하기 때문에 테이블도 생성해 줍니다.\n\n```java\nCREATE TABLE user(\n\tid BIGINT NOT NULL AUTO_INCREMENT,\n\tname VARCHAR(255),\n\tPRIMARY KEY(id)\n);\n```\n\n`Replication` 에 필요한 전용 계정을 생성해주고, `Replication`을 위한 권한을 부여해 줍니다. `root`를 사용할 경우 보안상 문제가 될 수 있기 때문에, 다음과 같이 `Slave` 권한을 부여해 줍니다.\n\n```java\nCREATE USER 'master'@'%' IDENTIFIED BY 'password';\nGRANT REPLICATION SLAVE ON *.* TO 'master'@'%';\n```\n\n그리고 MySQL 설정 변경을 위해 `my.cnf` 파일로 이동하여 값을 수정해줍니다.\n\n```bash\nsudo vim /etc/mysql/my.cnf\n```\n\n```java\n[mysqld]\nmax_allowed_packet=1000M\nserver-id = 1\nlog-bin = mysql-bin\nbinlog_format = ROW\nmax_binlog_size = 500M\nsync_binlog = 1\nexpire-logs-days = 7\nbinlog_do_db = test\n```\n\n설정들의 세부 값들은 다음과 같습니다.\n\n- max_allowed_packet : 서버로 질의하거나 받게되는 패킷의 최대 길이를 설정\n- server-id : 서버의 ID, 레플리케이션 토플리지(연결된 망)에서는 고유한 각각의 서버 ID를 가져야 함\n- log-bin : 바이너리 로그 파일 경로 → (/var/lib/mysql/mysql-bin.XXXXXX**)** 형식으로 저장\n- binlog_format : 바이너리 로그의 저장 형식을 지정. STATEMENT, ROW, MIXED 3가지 중 하나를 선택이 가능\n    - 해당 설정중 `ROW`를 선택. 그 이유는 현재 `InnoDB`를 사용 중이며, 트랜잭션 격리 수준이 `READ COMMITTED`일 경우 `ROW` 기반 로깅만 사용이 가능.\n- max_binlog_size : 바이너리 로그의 최대 크기\n- sync_binlog : N개의 트랜잭션 마다 바이너리 로그를 디스크에 동기화 시킬지 결정. 설정한 1은 안정적이지만, 가장 느린 설정임\n- expire-logs-days: 바이너리 로그가 만료되는 기간 설정\n- binlog_do_db : 레플리케이션을 적용할 데이터베이스 이름 설정\n\n설정이 끝났으면 MySQL을 재시작 해줍니다.\n\n```bash\nsudo systemctl restart mysql\n```\n\n그리고 `Master`의 상태를 확인해 잘 반영되었는지 확인합니다.\n\n```java\nSHOW MASTER STATUS:\n```\n\n![](img_9.png)\n\n> 중요!\n>\n\n레플리케이션에서는 이 `File`과 `Position` 값으로 `Master - Slave` 서버 동기화가 진행이 됩니다. 또한`File`과 `Position`은 데이터베이스에서 어떤 작업을 할 때마다 변경이 됩니다. `Master DB`의 해당 정보들을 `Slave DB`에서 기입해서 사용하기 때문에, 항상 확인 후 `Master - Slave` 연동을 진행해야 합니다.\n\n### Slave DB Server 작업\n\n이어서 위에서 설명했던 내용을 바탕으로 `Slave DB`를 설정해 줍니다.\n\n```java\nCREATE DATABASE test;\n```\n\n```java\nCREATE USER 'slave'@'%' IDENTIFIED BY 'password';\nGRANT ALL PRIVILEGES ON *.* TO 'slave'@'%';\n```\n\n```java\n$ vim /etc/mysql/my.cnf\n\n[mysqld]\n\nmax_allowed_packet=1000M\nserver-id = 2\nlog-bin  = mysql-bin\nbinlog_format   = ROW\nmax_binlog_size = 500M\nsync_binlog     = 1\nexpire-logs-days= 7\nslow_query_log = 1\nread_only = 1\n```\n\n설정을 변경했기 때문에 역시 MySQL을 재시작 해줍니다.\n\n```bash\nsudo systemctl restart mysql\n```\n\n그리고 최종적으로 `MySQL`에서 아래 쿼리를 실행해주면 됩니다. 여기서 `MASTER_LOG_FILE`과  `MASTER_LOG_POS` 값은 아까 `Master DB STATUS`에서 나온 값으로 설정을 하면 됩니다.\n\n```java\nRESET SLAVE;\n\nCHANGE MASTER TO MASTER_HOST='xxx.xxx.x.xxx', // private IP\n        MASTER_USER='master',\n        MASTER_PORT=8080,                     // 내부적인 이슈로 8080 포트로 연결\n        MASTER_PASSWORD='password',\n        MASTER_LOG_FILE='mysql-bin.00000x',\n        MASTER_LOG_POS=157;\n\nSTART REPLICA;\n```\n\n지금까지 잘 설정이 되었다면, 다음 명령어를 입력했을 때 문제가 없다는 것을 확인할 수 있습니다.\n\n``` java\nshow slave status\\G;\n```\n\n![](img_12.png)\n\n추가적으로 `Master DB` 서버에서 테이블 생성과 데이터 생성을 해보면 바로 `Slave DB` 서버에 잘 반영되는 것을 확인할 수 있습니다.\n\n[Master DB]\n\n![](img_10.png)\n\n[Slave DB]\n\n![](img_11.png)\n\n## SpringBoot 설정\n\nDB 서버를 `Master - Slave` 로 이중화 하였으므로, `SpringBoot`에서 사용하는 `DataSource`도 `Master - Slave`를 각각 사용해야 합니다. 이를 구현하기 위한 방법으로 `@Transactional` 애노테이션의 속성을 이용하여 `readOnly = false`인 트랜잭션은 `Master DataSource`를, `readOnly = true`인 트랜잭션은 `Slave DataSource`를 사용하도록 설정해 주겠습니다.\n\n### application.yml 설정\n\n저는 `Master 1`, `Slave 2` 를 바탕으로 `Replication`을 구성했기 때문에 3개의 `DataSource`에 대한 설정을 작성해주어야 합니다. 여기서 하나 유의해야 할 점은 일반적인 `DataSource` 등록 방법과는 과정이 조금 달라진다는 점입니다.\n\n하나의 `DataSource`만 사용하는 경우에는 `SpringBoot AutoConfiguration`을 통해서 자동으로 빈으로 만들어져 관리되었지만, 2개 이상의 `DataSource`를 사용하는 경우에는 개발자가 직접 빈을 만들어서 사용해야 합니다.\n\n```yaml\nspring:\n  datasource:\n    master:\n      username: master\n      password: password\n      driver-class-name: com.mysql.cj.jdbc.Driver\n      jdbc-url: jdbc:mysql://XXX.XXX.XXX.XXX:3306/test\n    slave1:\n      username: slave1\n      password: password\n      driver-class-name: com.mysql.cj.jdbc.Driver\n      jdbc-url: jdbc:mysql://XXX.XXX.XXX.XXX:3306/test\n\tslave2:\n      username: slave2\n      password: password\n      driver-class-name: com.mysql.cj.jdbc.Driver\n      jdbc-url: jdbc:mysql://XXX.XXX.XXX.XXX:3306/test\n```\n\n### DataSource 빈 등록\n\n```java\n@Configuration\npublic class DataSourceConfig {\n\n    private static final String MASTER_SERVER = \"MASTER\";\n    private static final String SLAVE1_SERVER = \"SLAVE1\";\n    private static final String SLAVE1_SERVER = \"SLAVE2\";\n\n    @Bean\n    @Qualifier(MASTER_SERVER)\n    @ConfigurationProperties(prefix = \"spring.datasource.master\")\n    public DataSource masterDataSource() {\n        return DataSourceBuilder.create()\n                .build();\n    }\n\n    @Bean\n    @Qualifier(SLAVE1_SERVER)\n    @ConfigurationProperties(prefix = \"spring.datasource.slave1\")\n    public DataSource slave1DataSource() {\n        return DataSourceBuilder.create()\n                .build();\n    }\n\n    @Bean\n    @Qualifier(SLAVE2_SERVER)\n    @ConfigurationProperties(prefix = \"spring.datasource.slave2\")\n    public DataSource slave2DataSource() {\n        return DataSourceBuilder.create()\n                .build();\n    }\n}\n```\n\n다음과 같이 각 DB 서버에 대응되는 `DataSource` 타입의 빈을 등록하면 됩니다.\n\n위의 코드를 보면, `@Qualifier` 애노테이션을 사용한 것을 확인할 수 있습니다. 일반적으로 같은 타입 (여기서는 DataSource)의 빈이 2개 이상 등록된 경우 스프링은 어떤 빈을 주입해야 하는지 모릅니다. 이를 해결하기 위해서 `@Qualifier` 애노테이션을 사용하여 **이름을 기반으로 한정자를 명시하여서, 주입받을 빈을 지정**할 수 있도록 만들면 됩니다.\n\n추가적으로 `@ConfigurationProperties` ****애노테이션을 사용한 것을 확인할 수 있습니다. 해당 애노테이션을 사용하면 `application.yml`에 명시한 여러 설정 중, 특정 `prefix`에 해당하는 설정 값을 자바 빈에 매핑할 수가 있습니다.\n\n### AbstractRoutingDataSource\n\n스프링은 `Multi DataSource` 환경에서 여러 `DataSource`를 묶고 분기해주기 위해서 `AbstractRoutingDataSource`라는 추상 클래스를 지원합니다.\n\n`AbstractRoutingDataSource` 를 살펴보면 다음과 같이 구성되어 있는 것을 확인할 수 있습니다.\n\n\n\n```java\npublic abstract class AbstractRoutingDataSource extends AbstractDataSource implements InitializingBean {\n\n\t@Nullable\n\tprivate Map<Object, Object> targetDataSources;\n\n\t@Nullable\n\tprivate Object defaultTargetDataSource;\n\n\tprivate boolean lenientFallback = true;\n\n\tprivate DataSourceLookup dataSourceLookup = new JndiDataSourceLookup();\n\n\t@Nullable\n\tprivate Map<Object, DataSource> resolvedDataSources;\n\n\t@Nullable\n\tprivate DataSource resolvedDefaultDataSource;\n\n\tpublic void setTargetDataSources(Map<Object, Object> targetDataSources) {\n\t\tthis.targetDataSources = targetDataSources;\n\t}\n\t...\n\n\tprotected DataSource determineTargetDataSource() {\n\t\tAssert.notNull(this.resolvedDataSources, \"DataSource router not initialized\");\n\t\tObject lookupKey = determineCurrentLookupKey();\n\t\tDataSource dataSource = this.resolvedDataSources.get(lookupKey);\n\t\tif (dataSource == null && (this.lenientFallback || lookupKey == null)) {\n\t\t\tdataSource = this.resolvedDefaultDataSource;\n\t\t}\n\t\tif (dataSource == null) {\n\t\t\tthrow new IllegalStateException(\"Cannot determine target DataSource for lookup key [\" + lookupKey + \"]\");\n\t\t}\n\t\treturn dataSource;\n\t}\n\t...\n}\n```\n\n위의 코드를 보면 `setTargetDataSources()`라는 메서드를 통해 `Map`을 전달합니다. 이때 `Map`의 `Value`로는 `DataSource`를 전달합니다. 전달된 `DataSource`는 `Map`의 `Key`를 통해서 찾을 수 있습니다.\n\n또한 `determineTargetDataSource()`메서드를 보면 실제로 사용된 `DataSource`를 결정합니다. 내부 코드를 확인해보면 `determineCurrentLookupKey()` 라는 메서드를 사용해서 가져올 `DataSource`의 `Key`를 결정합니다.\n\n실제 저희가 이를 활용하기 위해서 `AbstractRoutingDataSource` 를 상속받는 구체 클래스를 만들어서 위에서 언급한 `determineCurrentLookupKey()`메서드를 오버라이드하여 트랜잭션의 `readOnly` 값에 따라 다른 `DataSource` 의 `Key`를 반환하도록 구현하겠습니다.\n\n```java\npublic class RoutingDataSource extends AbstractRoutingDataSource {\n\n    private RoutingCircular<String> routingCircular;\n\n    @Override\n    public void setTargetDataSources(final Map<Object, Object> targetDataSources) {\n        super.setTargetDataSources(targetDataSources);\n\n        routingCircular = new RoutingCircular<>(\n                targetDataSources.keySet().stream()\n                        .map(Object::toString)\n                        .filter(key -> key.contains(\"slave\"))\n                        .collect(Collectors.toList())\n        );\n    }\t\t\n\n    @Override\n    protected Object determineCurrentLookupKey() {\n        boolean isReadOnly = TransactionSynchronizationManager.isCurrentTransactionReadOnly();\n\n        if (isReadOnly) {\n            return routingCircular.getOne();\n        }\n        return \"master\";\n    }\n} \n```\n\n해당 코드를 보면, `TransactionSynchronizationManager`의 `isCurrentTransactionReadOnly()`라는 메서드를 사용하였는데, 이를 통해서 현재 트랜잭션의 `read-only` 여부를 확인할 수 있습니다.\n\n### Slave DB의 다중화 처리\n\n위의 코드에서는 한가지 설명하지 않은 부분이 있습니다. 바로 `setTargetDataSources()`과 관련된 설명인데요. 저는 **`Slave DB`를 2개 이상 사용하려고 하기 때문에 `readOnly` 속성만으로는 이를 해결할 수 없습니다**. 이를 해결하기 위해서 `Slave DB`를 번갈아서 사용하도록 기능을 추가해주도록 하겠습니다.\n\n```java\npublic class RoutingCircular<T> {\n    private List<T> dataSources;\n    private Integer counter = 0;\n        \n    public RoutingCircular(final List<T> dataSources) {\n        this.dataSources = dataSources;\n    }\n\t\t\n    public T getOne() {\n        int circularSize = dataSources.size();\n        if (counter + 1 > circularSize) {\n            counter = 0;\n        }\n        return dataSources.get(counter++ % circularSize);\n    }\n}\n```\n\n다음과 같이 `RoutingCircular` 클래스를 만들어 통해서 **여러개의 `Slave DB`의 `DataSource`를 순서대로 로드밸런싱** 할 수 있게 됩니다.\n\n### 라우팅할 DataSource 등록하기\n\n지금까지 위에서 만든 `RoutingDataSource`에 우리가 사용할 3가지의 `DataSource` 정보를 등록하고, 이를 빈으로 만들어서 최종적으로 스프링에서 관리되도록 작업을 처리해주겠습니다. `@Qualifier`를 이용해서 어떤 빈을 주입받을 것인지 명확히 지시해서 작업을 진행합니다.\n\n```java\n@Bean\npublic DataSource routingDataSource(@Qualifier(MASTER_SERVER) DataSource masterDataSource,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t@Qualifier(SLAVE1_SERVER) DataSource slave1DataSource,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t@Qualifier(SLAVE2_SERVER) DataSource slave2DataSoucre) {\n    RoutingDataSource routingDataSource = new RoutingDataSource();\n\n    HashMap<Object, Object> dataSources = new HashMap<>();\n    dataSources.put(\"master\", masterDataSource);\n    dataSources.put(\"slave1\", slave1DataSource);\n    dataSources.put(\"slave2\", slave2DataSource);\n\n    routingDataSource.setTargetDataSources(dataSources);\n    routingDataSource.setDefaultTargetDataSource(masterDataSource);\n\n    return routingDataSource;\n}\n```\n\n해당 과정을 통해서 우리가 구현한 `RoutingDataSource`의 인스턴스를 생성한 뒤, 등록해 줄 각각의 `DataSource`를 `String Type`의 `Key`로 매핑하고, 기본 `DataSource`를 `Master`로 지정한 뒤 `Bean`으로 등록을 해줄 수 있습니다.\n\n그리고 실제 `Spring Boot`에서 사용하게 될 `DataSource` 를 만드는 작업을 수행하면 됩니다. **여기서 고려해야 할 점은 스프링이 `DataSource`를 통해 `Connection`을 획득하는 일련의 주기에 대해서 확인할 필요가 있습니다.**\n\n### Spring의 Connection 획득 방법과 LazyConnection 처리\n\n스프링은 트랜잭션에 진입하기만 하면 바로 `DataSource`를 가져와서 `Connection`을 가져옵니다. 말로만 하면 너무 추상적이니 Spring에서 Transactional을 처리하는 과정을 자세하게 한번 알아봅시다. 밑에 정리된 순서대로 Spring Transactional이 동작을 하게 됩니다.\n\n```java\n1. CglibAopProxy.DynamicAdvisedInterceptor.intercept( )\n\n2. TransactionInterceptor.invoke( )\n\n3. TransactionAspectSupport.invokeWithinTransaction( )\n\n4. TransactionAspectSupport.createTransactionIfNecessary( )\n\n5. AbstractPlatformTransactionManager.getTransaction( )\n\n6. AbstractPlatformTransactionManager.startTransaction( )\n\n7. AbstractPlatformTransactionManager.begin( )\n\n8. AbstractPlatformTransactionManager.prepareSynchronization( )\n\n9. TransactionAspectSupport.invokeWithinTransaction( )\n\n10. InvocationCallback.proceedWithInvocation( )\n\n11. @Transactional이 적용된 실제 타깃이 실행\n```\n\n우리가 여기서 꼭 확인해봐야 할 부분은 7번과 8번 과정입니다. Spring AOP는 7번 과정에서 `DataSource`로 부터 `Connection`을 가져오고 8번 과정에서 트랜잭션의 현재 상태를 `ThreadLocal`에 저장을 하게 됩니다. 즉, `TransactionSynchronizationManager`에 트랜잭션의 정보를 동기화 하는 작업이 `DataSource`로 부터 `Connection`을 가져온 이후에 실행이 된다는 것입니다.\n\n이때까지의 내용을 그림으로 정리하면 다음과 같습니다.\n\n![](img_5.png)\n\n하지만 저희는 Replication을 위해서 `AbstractRoutingDataSource`을 구현하여 트랜잭션 속성에 따라 `DataSource`를 선택해서 선택한 `DataSource`의 `Connection`객체를 리턴할 수 있도록 구현을 해야 했습니다.\n\n이러한 문제를 해결하기 위해서 `Spring`에서는 `LazyConnectionDataSourceProxy` 을 제공합니다. `Spring Docs`를 확인해보면 다음과 같은 내용을 확인할 수 있습니다.\n\n![](img_6.png)\n\n즉, 실제 쿼리를 호출하기 전까지 `JDBC Connection`을 가져오지 않고 `Connection Proxy`를 주고, 대상 메서드 안에서 쿼리가 실제 발생할 때 우리가 작성한 `AbstractRoutingDataSource` 에서 `Connection`을 얻어 쿼리를 실행하도록 로직을 구현할 수 있습니다.\n\n설명이 굉장히 길었는데요. 이제 밑의 코드와 같이 설정을 해주면 저희가 원하는데로 `DataSource`를 가져올 수 있게 됩니다.\n\n```java\n@Bean\n@Primary\npublic DataSource dataSource() {\n    DataSource determinedDataSource = routingDataSource(masterDataSource(), slave1DataSource(), slave2DataSource());\n    return new LazyConnectionDataSourceProxy(determinedDataSource);\n}\n```\n\n![](img_7.png)\n\n여기서 `@Primary` 애노테이션을 작성해준 것을 확인할 수 있는데, 기본적으로 `Spring Boot`의 `AutoConfiguration`은 한 개의 `DataSource`를 가정하고 설정이 되어 있습니다. 현재는 여러 개의 `DataSource`를 사용하는 경우이기 때문에, `@Primary` 애노테이션을 붙여주어서 `DataSource`에 대한 지정을 해주어야 합니다.\n\n지금까지 과정이 성공적으로 수행이 되었다면 `Replication`을 정상적으로 동작시킬 수 있습니다.\n\n## Replication 설계를 하면서 고려한 문제들과 앞으로의 개선방향\n\n### 데이터 정합성 문제\n\n이번 작업을 하면서 가장 고민을 했던 부분입니다.  만약 실시간으로 계속 쿠폰이 발행되고, 한쪽에서는 계속 조회되는 환경이라면 동기화 이전 시점에 조회를 하게 될 경우 `NPE`가 발생하게 될 가능성은 남아있었습니다. 이를 해결하기 위해 다양한 방법을 고민해보았는데 2가지 정도의 방법을 찾을 수 있었습니다.\n\n첫번째 방법은 **반동기(Semi Async)** 방식입니다. `MySQL Docs`를 살펴보면 **비동기(Async)** 방식이 아닌 **반동기(Semi Async)** 방식을 사용할 수 있다고도 나와있습니다. 해당 방법을 통해서 DB의 성능을 조금 포기하고, `Master - Slave`의 동기화를 조금 더 보장해줍니다. 하지만 이 방식은 `Master`가 특정 세션의 트랜잭션을 처리하기 위해서 `Slave`들 중 적어도 1개가 `ACK` 하거나 `timed out`에 도달할 때까지 기다리기 때문에 성능에 악영향을 미쳐 `Replication`의 이점을 살리지 못한다는 생각이 들었습니다.\n\n두번째 방법은 **타입 스탬프** 방식입니다. 최근 N초 ~ N분 까지의 갱신 내역은 `Master`에서 읽고, 이후 내역은 `Slave` 에서 읽는 방법이라고 할 수 있습니다. 이 방법은 `Master`에 트래픽이 많이 몰려 `Master`의 장애 위험이 높다는 생각이 들었습니다.\n\n결론적으로 저는 지금처럼 `MySQL Replication`에서 기본적으로 제공하는 **비동기** 방식으로 처리를 하고, 데이터 불일치 현상을 다른 방식으로 해결하기로 결론을 내렸습니다.\n\n현재 생각하는 해결방법은 **정합성이 중요한 로직에서는 조회 요청 역시 `Master DB`를 타게끔 설정하는 것** 입니다. 물론 `Master DB`의 부하가 생기겠지만, `Replication`을 진행하지 않았을 때보다는 훨씬 부하를 분산시킬 수 있기 때문에 현재 가용할 수 있는 최대한의 `Scale - up`을 한 후, `Master DB`를 통해 정합성 문제를 처리하기로 결론을 내렸습니다.\n\n### Master 장애시 Slave의 승격 여부\n\n만약 `Master`가 장애가 난다면 `Slave` 하나를 승격시켜서 이를 `Master` 처럼 사용해야 할 것입니다. 지금 구조에서 문제라고 생각하는 점은 `Master` 장애시 관리자가 수동으로 이를 처리해야 한다는 점입니다.\n\n이러한 문제를 `Group Replicaiton`으로 해결해줄 수 있다고 합니다.\n\n![](img_8.png)\n\n`Group Replication`을 사용하면 `Master DB` 장애시 자동으로 `Slave DB`가 `Master DB`로 승격되는 장점이 있습니다. 이러한 이유 때문에 현재 구조인 `Master - Slave Replication` 구조에서 `Group Replication`으로 변경하는 것이 조금 더 장애 대처에 좋을 것이라 생각이 드는데 정확한 내용은 조금 더 학습을 해봐야 할 것으로 보입니다.\n\n\n## Replication 작업 과정에서 겪을 수도 있는 문제들\n\n### Public key retrieval is not allowed\n\nMySQL DB에 접속하려면 기본적으로 url, username, password 3가지 옵션이 필요합니다. 하지만 MySQL 8.0 버전부터는 보안적인 이슈로 `useSSL` 옵션에 대한 추가적인 설정이 필요합니다.\n\n다음 두 가지 옵션이 필요한데 이는 다음과 같습니다.\n\n- useSSL : DB에 SSL로 연결\n- allowPublicKeyRetrieval: 클라이언트가 서버에서 공개키를 자동으로 요청할 수 있도록 설정\n\n최종적으로 `application.yml`에 2가지 속성을 추가해주면 정상적으로 연결이 됩니다.\n\n```yaml\njdbc:mysql://xxx.xxx.xx.x:3306/test_db?useSSL=false&allowPublicKeyRetrieval=true\n```\n\n### caching_sha2_password authentication plugin\n\nMySQL 8.0은 `SHA-246 hasing`을 구현하는 두 가지 인증 플러그인을 지원합니다.\n\n- sha256_password : 기본적인 SHA-256 인증을 구현한 플러그인\n- caching_sha2_password : sha256_password와 동일한데 성능 향상을 위해 서버 캐싱을 이용\n\n문제는 `caching_sha2_password`를 사용하려면 다음 조건 중 하나가 만족해야 한다는 점입니다.\n\n- SSL 보안연결을 사용\n- RSA 보안을 적용한 비암호 연결을 사용\n\n하지만 저는 테스트 용으로 로컬에서 작업을 하고 있었기 때문에 보안 연결을 사용하지 않아 아래와 같은 문제가 발생했었습니다.\n\n```java\nLast_IO_Error: error connecting to master 'replication@test:3306' - retry-time: 60 retries: 1 message: Authentication plugin 'caching_sha2_password' reported error: Authentication requires secure connection.\n```\n\n이를 해결하는 방법은 이전의 `mysql_native_password`를 사용하면 됩니다. 사용자의 암호를 `DDL`을 통해서 이전 방식으로 변경할 수 있습니다.\n\n```bash\nmysql> ALTER USER 'replication'@'%' IDENTIFIED WITH mysql_native_password BY 'password';\n```\n\n그리고 MySQL을 재시작하면 정상적으로 MySQL에 접속할 수 있는 것을 확인할 수 있습니다.\n\n단, 해당 방식은 MySQL이 지원하는 예전 방식이기 때문에 실제로 운영환경에서는 보안 작업을 처리한 후, 사용하는게 좋을 것으로 보입니다.\n\n\n## 마무리\n\n지금까지 MySQL의 Replication 과 Spring Boot에서의 활용방법에 대해서 학습한 내용과 고민한 부분을 작성해보았습니다. (추가적으로 성능 테스트 결과를 보충해서 작성할 예정입니다) 위에서 소개된 방법들이 정답은 아니기 때문에, 앞으로도 조금 더 DB Scale-out에 대해서 고민을 해봐야겠습니다.\n\n> 참고한 곳\n>\n- [https://docs.spring.io/spring-framework/docs/4.2.x/spring-framework-reference/html/transaction.html](https://docs.spring.io/spring-framework/docs/4.2.x/spring-framework-reference/html/transaction.html)\n- [https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/jdbc/datasource/LazyConnectionDataSourceProxy.html](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/jdbc/datasource/LazyConnectionDataSourceProxy.html)\n- [https://ssup2.github.io/theory_analysis/MySQL_Replication/](https://ssup2.github.io/theory_analysis/MySQL_Replication/)\n- [https://backtony.github.io/spring/mysql/aws/2021-09-28-spring-mysql-1/](https://backtony.github.io/spring/mysql/aws/2021-09-28-spring-mysql-1/)\n- [https://trandent.com/article/etc/detail/320833](https://trandent.com/article/etc/detail/320833)\n"},{"excerpt":"들어가기전.. Spring AOP는 스프링의 3대 축이라고 불리기도 하지만 그만큼 내용의 깊이가 깊고 어려운 항목이기도 합니다. 이로 인해서, Spring AOP를 계속 사용하고 있음에도 불구하고 개발자는 인지하지 못하기도 합니다. 이번 시리즈에서는 Spring AOP를 조금씩 알아보는 시간을 가져보려고 합니다. 프록시 패턴과 데코레이터 패턴의 차이..?…","fields":{"slug":"/다이나믹-프록시와-팩토리-빈/"},"frontmatter":{"date":"October 15, 2022","title":"다이나믹 프록시와 팩토리 빈","tags":["AOP"]},"rawMarkdownBody":"\n## 들어가기전..\n\nSpring AOP는 스프링의 3대 축이라고 불리기도 하지만 그만큼 내용의 깊이가 깊고 어려운 항목이기도 합니다. 이로 인해서, Spring AOP를 계속 사용하고 있음에도 불구하고 개발자는 인지하지 못하기도 합니다. 이번 시리즈에서는 Spring AOP를 조금씩 알아보는 시간을 가져보려고 합니다.\n\n## 프록시 패턴과 데코레이터 패턴의 차이..?\n\n이 두가지 패턴의 차이점을 알기 전에 프록시의 개념에 대해서 간단하게 살펴보겠습니다.\n\n프록시는 클라이언트라 사용하려고 하는 실제 대상인 것처럼 위장해서 클라이언트의 요청을 받아주는 대리자, 대리인과 같은 역할을 한다고 해서 **프록시**라고 부릅니다. 그리고 이러한 프록시를 통해 최종적으로 요청을 처리하는 실제 오브젝트를 **타깃**이라고 부릅니다. 다음 그림은 일반적인 **클라이언트 - 프록시 - 타깃의 구조**입니다.\n\n![](img.png)\n\n이러한 프록시는 사용 목적에 따라서 두 가지로 구분할 수 있습니다. 첫째는 클라이언트가 타깃에 접근하는 방법을 제어하기 위해서 사용될 수 있습니다. 두번째는 타깃에 부가적인 기능을 부여해주기 위해서 사용될 수 있습니다. 무엇이 프록시 패턴이고 데코레이터 패턴일까요?? 하나씩 살펴보도록 하겠습니다.\n\n### 데코레이터 패턴\n\n데코레이터 패턴은 타깃에 부가적인 기능을 런타임 시 다이내믹하게 부여해주기 위해 프록시를 사용하는 패턴을 말합니다. 데코레이터 패턴은 이름에서도 의미를 유추할 수 있듯이 **하나의 타깃을 여러 겹으로 포장하고 장식을 붙이는데 사용**이 됩니다. 즉, **프록시가 한 개 이상이 될 수 있음을 의미**합니다.\n\n이러한 **데코레이터는 위임하는 대상에게도 인터페이스로 접근을 하기 때문에, 자신이 최종 타깃으로 위임을 하는지 또 다른 데코레이터 프록시로 위임을 하는지 알 수 없습니다**. 즉, 외부에서 위임을 할 대상을 런타임 시에 주입받을 수 있도록 만들어야 합니다.\n\n![](img_1.png)\n\n이러한 데코레이터 패턴은 타깃의 코드를 손대지 않고, 클라이언트가 호출하는 방법도 변경하지 않은 채로 새로운 기능을 추가할 때 유용한 방법이라고 할 수 있습니다.\n\n### 프록시 패턴\n\n프록시 패턴은 타깃의 기능을 확장하거나 추가하지 않습니다. **대신 클라이언트가 타깃에 접근하는 방식을 변경**해줍니다. 예를 들면, 타깃 오브젝트를 생성하기가 복잡하거나 당장 필요하지 않은 경우, 필요한 시점까지 오브젝트를 생성하지 않는 것이 좋습니다. 하지만 **타깃 오브젝트에 대한 레퍼런스가 미리 필요할 때가 있습니다**. 이럴 때 프록시 패턴을 사용할 수 있습니다.\n\n프록시 패턴은 클라이언트에게 **실제 타깃에 대한 레퍼런스를 넘기는 것이 아닌, 프록시를 넘겨주는 것**을 의미합니다. 그리고 **프록시의 메서드를 통해 타깃을 사용하려고 시도하면, 그때 타깃 오브젝트를 생성해서 요청을 위임하여 객체의 생성을 최대한 늦춥**니다.\n\n![](img_2.png)\n\n이러한 프록시 패턴은 객체의 생성을 요청시점까지 최대한 늦춤으로써 끝까지 사용하지 않거나, 많은 작업이 진행된 후에 사용되는 비효율적인 생성을 보다 효율적으로 사용할 수 있도록 하는 유용한 방법이라고 할 수 있습니다.\n\n## 다이내믹 프록시\n\n프록시는 기존 코드에 영향을 주지 않으면서 타깃의 기능을 확장하거나 접근 방법을 제어할 수 있는 유용한 방법입니다. 하지만 해당방법은 상당히 귀찮은 작업이라고도 할 수 있습니다. **매번 새로운 클래스를 정의해야하고, 인터페이스 처리된 메서드들에 대해서 위임하는 작업들을 전부 해줘야 하기 때문**입니다.\n\n이렇게 불편한 작업들을 우리는 `java.lang.reflect`에 있는 패키지에서 프록시를 손쉽게 만들 수 있도록 기능을 제공하고 있습니다.\n\n### 프록시의 구성과 프록시 작성의 문제점\n\n프록시는 다음의 두 가지 기능으로 구성됩니다.\n\n- 타깃과 같은 메서드를 구현하고 있다가 메소드가 호출되면 타킷 오브젝토로 위임한다.\n- 지정된 요청에 대해서는 부가기능을 수행한다.\n\n이렇게 프록시를 만들게 되면 다음과 같이 **위에서 말한 귀찮은 작업**들을 매번 해줘야 합니다.\n\n- 타겟의 인터페이스를 구현하고 위임하는 코드를 작성하기가 번거롭다. 부가기능이 필요없는 메서드도 이를 적용해야 한다. 또한, 타겟의 인터페이스 메서드가 추가되거나 변경될 때마다 함께 수정해줘야 한다.\n- 부가기능 코드가 중복될 가능성이 많다. 트랜잭션은 DB를 사용하는 대부분의 로직에 적용될 필요가 있다. 예를들어 add()에 부가 기능을 추가해주고, update()에도 부가 기능을 추가해줘야 한다면 트랜잭션 기능을 제공하는 유사한 코드가 여러 매서드에 중복되서 나타날 것이다.\n\n이러한 문제를 어떻게 해결해야 할까요? 바로 **JDK 다이내믹 프록시**를 통해서 해결할 수 있습니다.\n\n### 리플랙션\n\n다이내믹 프록시는 리플렉션 기능을 이용해서 만들 수 있습니다. 리플랙션을 간단히 설명하자면 자바의 코드 자체를 추상화에서 접근하도록 만든 것이라고 할 수 있습니다.\n\n자바의 모든 클래스는 그 클래스 자체의 구성정보를 담은 `Class` 타입의 오브젝트를 하나씩 갖고 있습니다. 이러한 `Class` 타입의 오브젝트를 이용하면 클래스 코드에 대한 메타정보를 가져오거나 오브젝트를 조작할 수 있습니다.\n\n조금 자세하게 예를 들어서 설명하면 `String.class`가 가진 `length()`라는 메서드의 정보를 가져오기 위해서는 다음과 같이 메서드 정보를 가져올 수 있습니다.\n\n```java\nMethod lengthMethod = String.class.getMethod(\"length\");\n```\n\n그리고 이렇게 가져온 메서드의 정보를 가지고 실행도 시킬 수 있습니다.\n\n```java\nint length = lengthMethod.invoke(name);\n```\n\n리플랙션 사용까지 알았다면 다이내믹 프록시를 적용하는데 큰 어려움이 없을 것 입니다.\n\n### 다이내믹 프록시를 적용해보자\n\n다이내믹 프록시는 **프록시 팩토리에 의해 런타임 시 다이내믹하게 만들어지는 오브젝트**를 말합니다. 다이내믹 프록시 오브젝트는 타깃의 인터페이스와 같은 타입으로 만들어집니다. 이 덕분에 **프록시를 만들 때, 인터페이스를 모두 구현해가면서 클래스를 정의하지 않아도 됩니다**.\n\n여기서 개발자는 다이내믹 프록시를 통해서 만들어주는 오브젝트를 기반으로, 제공하려는 부가기능 제공 코드만 작성을 하면 됩니다. 이런 부가기능 제공 코드는 `InvocationHandler`를 구현한 오브젝트에 담으면 됩니다.\n\n```java\npublic Object invoke(Object proxy, Method method, Object[] args)\n```\n\n`InvocationHandler`의 유일한 메서드인 `invoke()` 메서드는 리플레션의 `Method`와 메서드를 호출할 때 전달되는 파라미터도 `args`로 받습니다. 즉, 다이내믹 프록시 오브젝트는 **클라이언트의 모든 요청을 리플렉션 정보로 변환해서 `InvocationHandler` 를 구현한 오브젝트의 `invoke()` 메서드로 넘긴다**고 할 수 있습니다.\n\n코드를 통해서 이를 한번 확인해보겠습니다.\n\n```java\npublic class TxHandler implements InvocationHandler {\n\n    private Object target;\n    private PlatformTransactionManager transactionManager;\n\n    public TxHandler(Object target, PlatformTransactionManager transactionManager) {\n        this.target = target;\n        this.transactionManager = transactionManager;\n    }\n\n    @Override\n    public Object invoke(final Object proxy, final Method method, final Object[] args) throws Throwable {\n        TransactionStatus transactionStatus = transactionManager.getTransaction(new DefaultTransactionDefinition());\n        try {\n            Object result = method.invoke(target, args);\n            transactionManager.commit(transactionStatus);\n            return result;\n        } catch (InvocationTargetException e) {\n            transactionManager.rollback(transactionStatus);\n            throw e.getTargetException();\n        }\n    }\n}\n```\n\n중요한 내용이라서 흐름을 한번 더 정리해보겠습니다.\n\n1. 다이내믹 프록시로부터 요청을 전달받으려면 `InvocationHandler`를 구현해야 합니다. 즉, 다이내믹 프록시가 클라이언트로부터 받는 모든 요청은 `invoke()` 메서드로 전달이 됩니다.\n2. 이렇게 전달이 되면 **리플렉션 API**를 이용해서 **타깃 오브젝트**의 메서드를 호출합니다.\n3. 이후 지정한 부가기능을 수행하고, 결과를 리턴하면 해당 값을 **다이내믹 프록시**가 받아서 최종적으로 클라이언트에게 전달합니다.\n\n이러한 프록시는 다음과 같이 생성할 수 있습니다.\n\n```java\nfinal var txHandler= new TxHandler(target, transactionManager);\nfinal var userService = (UserService) Proxy.newProxyInstance(\n     getClass().getClassLoader(),\n\t   new Class[]{UserService.class},\n     txHandler);\n```\n\n이제 우리는 **다이내믹 프록시**를 이용해서 조금 더 깔끔하게 `Transactional` 을 적용할 수 있게 되었습니다.\n\n해당 내용을 구현한 코드는 다음 링크에서 확인할 수 있습니다. → [링크](https://github.com/woowacourse/jwp-dashboard-jdbc/pull/177/commits/76e7dc9bfb17be6275378bbd7039a29430dd6242)\n\n## 팩토리 빈\n\n위의 내용들을 통해서 조금 더 깔끔하게 `Transactional`을 사용할 수 있는 코드를 만들었습니다. 그렇다면 이를 **스프링의 DI** 통해 사용할 수 있도록 만들어 볼 수 있을 것 같습니다. 하지만 **DI의 대상이 되는 다이내믹 프록시 오브젝트는 일반적인 스프링 빈으로 등록이 불가능**합니다.\n\n스프링의 빈은 기본적으로 클래스 이름과 프로퍼티로 정의됩니다. 이를 기반으로 스프링은 지정된 클래스 이름을 가지고 리플렉션을 통해서 해당 클래스의 오브젝트를 만듭니다.\n\n```java\nDate now = (Date) Class.forName(\"java.util.Date\").newInstance();\n```\n\n여기서 문제는 **다이내믹 프록시는 이런 식으로 프록시 오브젝트가 생성되지 않는다는 점**입니다. 그렇다면 방법이 없는 것일까요?? 스프링은 무언가 다른 방법을 제시해줄 것 같습니다.\n\n### 팩토리 빈\n\n팩토리 빈은 클래스 정보를 기반으로 디폴트 생성자를 통해 오브젝트를 만드는 방법 외에 특별하게 빈을 만들 수 있는 방법 중 하나입니다. 여러가지 방법이 있지만 가장 간단한 방법은 스프링의 `FactoryBean` 인터페이스를 구현하는 것 입니다.\n\n```java\npublic interface FactoryBean<T> {\n    T getObject() throws Exception;     // 빈 오브젝트를 생성해서 돌려준다. \n    Class<? extends T> getObjectType(); // 생성되는 오브젝트의 타입을 알려준다.\n    boolean isSingleton();              // getObject()가 돌려주는 오브젝트가 항상 같은 싱글톤 오브젝트인지 알려준다.\n}\n```\n\n스프링은 `FactoryBean` 인터페이스를 구현한 클래스가 빈의 클래스로 지정되면, `FactoryBean` 클래스의 오브젝트의 `getObject()`를 이용해 오브젝트를 가져오고, 이를 빈 오브젝트로 사용합니다.\n\n### 다이내믹 프록시를 팩토리 빈으로 만들어보자\n\n이러한 팩토리 빈을 통해서 다이내믹 프록시를 만들 수 있습니다. 팩토리 빈의 `getObject()` 메서드에 다이내믹 오브젝트를 만들어주는 코드를 작성하면 되기 때문입니다.\n\n코드를 통해서 이를 확인해보겠습니다.\n\n```java\npublic class TxProxyFactoryBean implements FactoryBean<Object> { // 범용적으로 사용하기 위해 Object로 지정했다.\n    \n    Object target;\n    PlatformTransactionManager transactionManager;\n    String pattern;\n    Class<?> serviceInterface; // 다이내믹 프록시를 생성할 때 필요하다.\n\n    public Object getObject() throws Exception {\n        TxHandler txHandler = new TxHandler();\n        txHandler.setTarget(target);\n        txHandler.setTransactionManager(transactionManager);\n        txHandler.setPattern(pattern);\n\n        return Proxy.newProxyInstance(getClass().getClassLoader(), new Class[] { serviceInterface }, txHandler);\n    }\n\n    public Class<?> getObjectType() {\n        return serviceInterface; // 팩토리 빈이 생성하는 오브젝트의 타입은 DI 받은 인터페이스 타입에 따라 달라진다. 따라서 다양한 타입의 프록시 오브젝트 생성에 재사용할 수 있다.\n    }\n\n    public boolean isSingleTon() {\n        return false; // 싱글톤 빈이 아니라는 뜻이 아니라 getObject()가 매번 같은 오브젝트를 리턴하지 않는다는 뜻이다.\n    }\n\n    // setter ..\n}\n```\n\n위의 코드를 보면 알 수 있듯이 팩토리 빈이 만드는 다이내믹 프록시는 구현 인터페이스나, 타깃의 종류에 제한이 없습니다. 따라서 **DI를 통해 TxHandler를 이용하는 다이내믹 프록시를 재사용해서 사용**할 수 있습니다.\n\n### 팩토리 빈의 한계\n\n`TransactionHandler`를 이용하는 다이내믹 프록시를 생성해주는 `TxProxyFactoryBean`은 코드의 수정 없이도 다양한 클래스에 적용이 가능합니다. 타깃 오브젝트에 맞는 프로퍼티 정보만 설정해서 빈으로 등록해주면 되기 때문입니다.\n\n하지만 이러한 방법 역시 명확한 한계점을 가지고 있습니다. 바로 한 번에 여러 클래스에 공통적인 부가기능을 제공하는 것 입니다. 지금과 같은 방법은 **적용 대상인 클래스가 N개라면 그만큼의 프록시 패턴의 빈의 설정이 중복**되는 구조를 가지게 됩니다.\n\n또한 **`Handler` 오브젝트가 프록시 팩토리 빈 갯수만큼 만들어진다는 점입니다.** 이는 타깃 오브젝트르 프로퍼티로 가지고 있음으로 생기는 문제입니다.\n\n이러한 문제들을 해결할 수 있는 방법은 없을까요?? 이를 다음글에서 한번 살펴보도록 합시다.\n\n## 글을 마무리하며..\n\n이번 글을 정리하면서 Proxy에 대해서 다시 한번 정리할 수 있는 시간이였습니다. 매번 해당 내용이 추상적으로 다가왔는데 이번에는 조금 이해가 잘되었던 것 같습니다. 아마 진행하고 있는 테크코스 과제와 스터디가 큰 영향을 미친거 같기도 합니다. 이번 6장을 끝으로 토비의 스프링 스터디를 마무리 할 것 같은데 유의미하게 마무리할 수 있었으면 좋겠습니다.\n\n<br>\n\n> 참고한 글\n>\n- 토비의 스프링 3.1\n- [https://github.com/woowacourse/jwp-dashboard-jdbc](https://github.com/woowacourse/jwp-dashboard-jdbc)\n- [https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/InvocationHandler.html](https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/InvocationHandler.html)"}]}},"pageContext":{}},"staticQueryHashes":[]}